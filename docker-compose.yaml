services:
  namenode:
    networks:
      - spark_mts
    image: mirror.gcr.io/apache/hadoop:3.3.6
    hostname: namenode
    user: root
    command: >
      bash -c "
        if [ ! -d /hadoop/dfs/name/current ]; then
          echo 'Formatting NameNode...'
          hdfs namenode -format -force
        fi &&
        hdfs namenode
      "
    ports:
      - 9870:9870
    env_file:
      - ./config.env
    volumes:
      - namenode_data:/hadoop/dfs/name
    environment:
      ENSURE_NAMENODE_DIR: "/hadoop/dfs/name"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode1:
    networks:
      - spark_mts
    ports:
      - 9864:9864
    user: root
    image: mirror.gcr.io/apache/hadoop:3.3.6
    command: ["hdfs", "datanode"]
    volumes:
      - datanode1_data:/hadoop/dfs/data
    env_file:
      - ./config.env
    depends_on:
      namenode:
        condition: service_healthy

  datanode2:
    networks:
      - spark_mts
    ports:
      - 9865:9864  # Different host port
    user: root
    image: mirror.gcr.io/apache/hadoop:3.3.6
    command: ["hdfs", "datanode"]
    volumes:
      - datanode2_data:/hadoop/dfs/data
    env_file:
      - ./config.env
    depends_on:
      namenode:
        condition: service_healthy

  # spark-master:
  #   image: bde2020/spark-master:3.3.0-hadoop3.3
  #   container_name: spark-master
  #   networks:
  #     - spark_mts
  #   ports:
  #     - "8080:8080"
  #     - "7077:7077"
  #   environment:
  #     - INIT_DAEMON_STEP=setup_spark
  #   env_file:
  #     - ./config.env
      
  # spark-worker-1:
  #   image: bde2020/spark-worker:3.3.0-hadoop3.3
  #   container_name: spark-worker-1
  #   networks:
  #     - spark_mts
  #   depends_on:
  #     - spark-master
  #   ports:
  #     - "8081:8081"
  #   environment:
  #     - "SPARK_MASTER=spark://spark-master:7077"
  #     - "SPARK_WORKER_WEBUI_PORT=8081"
  #   env_file:
  #     - ./config.env

  # spark-worker-2:
  #   image: bde2020/spark-worker:3.3.0-hadoop3.3
  #   container_name: spark-worker-2
  #   networks:
  #     - spark_mts
  #   depends_on:
  #     - spark-master
  #   ports:
  #     - "8082:8082"
  #   environment:
  #     - "SPARK_MASTER=spark://spark-master:7077"
  #     - "SPARK_WORKER_WEBUI_PORT=8082"
  #   env_file:
  #     - ./config.env

  # spark-history-server:
  #   image: bde2020/spark-history-server:3.3.0-hadoop3.3
  #   container_name: spark-history-server
  #   networks:
  #     - spark_mts
  #   depends_on:
  #     - spark-master
  #   ports:
  #     - "18081:18081"
  #   volumes:
  #     - spark-events:/tmp/spark-events
  #   env_file:
  #     - ./config.env

  # hive-metastore-postgres:
  #   networks:
  #     - spark_mts
  #   image: postgres:11
  #   hostname: hive-metastore-postgres
  #   ports:
  #     - "5432:5432"
  #   environment:
  #     - POSTGRES_USER=hive
  #     - POSTGRES_PASSWORD=hive
  #     - POSTGRES_DB=metastore
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U hive"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5

  # hive-metastore:
  #   networks:
  #     - spark_mts
  #   image: apache/hive:3.1.3
  #   hostname: hive-metastore
  #   ports:
  #     - "9083:9083"
  #   volumes:
  #     - hive_metastore_data:/opt/hive/data/warehouse
  #   environment:
  #     - SERVICE_NAME=metastore
  #   env_file:
  #     - ./config.env
  #   command: >
  #     bash -c "
  #       /opt/hive/bin/schematool -dbType postgres -initSchema || true &&
  #       /opt/hive/bin/hive --service metastore
  #     "
  #   depends_on:
  #     hive-metastore-postgres:
  #       condition: service_healthy
  #     namenode:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ["CMD", "nc", "-z", "localhost", "9083"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # hiveserver2:
  #   networks:
  #     - spark_mts
  #   image: apache/hive:3.1.3
  #   hostname: hiveserver2
  #   ports:
  #     - "10000:10000"  # HiveServer2 Thrift port
  #     - "10002:10002"  # HiveServer2 Web UI
  #   volumes:
  #     - hive_server_data:/opt/hive/data/warehouse
  #   environment:
  #     - SERVICE_NAME=hiveserver2
  #   env_file:
  #     - ./config.env
  #   depends_on:
  #     hive-metastore:
  #       condition: service_healthy
  #     namenode:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ["CMD", "nc", "-z", "localhost", "10000"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

  # jupyter:
  #   build:
  #     context: .
  #     dockerfile: dockerfile.jupyter
  #   networks:
  #     - spark_mts
  #   hostname: jupyter
  #   ports:
  #     - 8888:8888
  #     - 4040:4040
  #     - 4041:4041
  #   environment:
  #     SPARK_MASTER: spark://spark-master:7077
  #     HADOOP_CONF_DIR: /etc/hadoop
  #     HADOOP_USER_NAME: hadoop
  #   volumes:
  #     - ./hadoop-config:/etc/hadoop:ro
  #     - ./notebooks:/home/jovyan/work
  #   depends_on:
  #     - spark-master
  #     - namenode
  #     - hive-metastore
  #     - hiveserver2

networks:
  spark_mts:

volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:
  postgres_data:
  hive_metastore_data:
  hive_server_data:
  spark-events:
