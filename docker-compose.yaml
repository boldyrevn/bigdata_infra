services:
  namenode:
    networks:
      - spark_mts
    image: mirror.gcr.io/apache/hadoop:3.2.1
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
    env_file:
      - ./config
    volumes:
      - namenode_data:/hadoop/dfs/name
    environment:
        ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode1:
    networks:
      - spark_mts
    ports:
      - 8042:8042
    image: mirror.gcr.io/apache/hadoop:3.2.1
    command: ["hdfs", "datanode"]
    volumes:
      - datanode1_data:/hadoop/dfs/data
    env_file:
      - ./config

  datanode2:
    networks:
      - spark_mts
    image: mirror.gcr.io/apache/hadoop:3.2.1
    command: ["hdfs", "datanode"]
    volumes:
      - datanode2_data:/hadoop/dfs/data
    env_file:
      - ./config

  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    env_file:
      - ./config
      
  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    env_file:
      - ./config

  spark-worker-2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"

  spark-history-server:
      image: bde2020/spark-history-server:3.3.0-hadoop3.3
      container_name: spark-history-server
      depends_on:
        - spark-master
      ports:
        - "18081:18081"
      volumes:
        - spark-events:/tmp/spark-events

  hive-metastore-postgres:
    networks:
      - spark_mts
    image: postgres:11
    hostname: hive-metastore-postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
      - POSTGRES_DB=metastore
    volumes:
      - postgres_data:/var/lib/postgresql/data

  hive-metastore:
    networks:
      - spark_mts
    image: apache/hive:3.1.3
    hostname: hive-metastore
    ports:
      - "9083:9083"
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres
      - SERVICE_OPTS=-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://hive-metastore-postgres:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive
    volumes:
      - hive_metastore_data:/opt/hive/data/warehouse
    depends_on:
      - hive-metastore-postgres
      - namenode

  jupyter:
    build:
      context: .
      dockerfile: dockerfile.jupyter
    networks:
      - spark_mts
    hostname: jupyter
    ports:
      - 8888:8888
      - 4040:4040  # Spark UI
      - 4041:4041  # Дополнительный Spark UI (если запускаете несколько приложений)
    environment:
      SPARK_MASTER: spark://spark-master:7077
      HADOOP_CONF_DIR: /etc/hadoop
      HADOOP_USER_NAME: hadoop
    volumes:
      - ./hadoop-config:/etc/hadoop:ro
      - ./notebooks:/home/jovyan/work
    depends_on:
      - spark-master
      - namenode
      - hive-metastore

networks:
  spark_mts:

volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:
  postgres_data:
  hive_metastore_data:
  hive_server_data:
  spark-events:
