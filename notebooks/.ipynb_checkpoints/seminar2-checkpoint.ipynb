{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3bb18b-2eb3-4a01-bfc7-6ce026871e65",
   "metadata": {},
   "source": [
    "# MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8415c246-2c35-41f6-80e3-578493f34775",
   "metadata": {},
   "source": [
    "### Что такое MapReduce\n",
    "\n",
    "**MapReduce** — это модель параллельных вычислений и фреймворк для обработки больших объёмов данных (Big Data).\n",
    "Она была предложена Google в статье *“MapReduce: Simplified Data Processing on Large Clusters”* (2004).\n",
    "Основная идея — разделить обработку данных на две стадии:\n",
    "\n",
    "1. **Map (отображение)** — применяет функцию ко всем входным данным и преобразует их в пары (ключ, значение).\n",
    "\n",
    "2. **Reduce (свёртка)** — агрегирует результаты по ключам, полученным на предыдущем шаге."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e54a2-bc7d-4aa7-a18f-38c34744bfc9",
   "metadata": {},
   "source": [
    "![MapReduce](mr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7bb06-e689-469b-bfe7-3b3e3ee68e6e",
   "metadata": {},
   "source": [
    "### Скачаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c50d2-4d55-462a-9d9e-2b46a42cbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir seminar-2-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f62fb3-7e30-48fc-ad84-19ccfd2f54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd seminar-2-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9b970-9ae5-4873-a0c0-b256a6a38fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -o tweets.csv https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/refs/heads/master/IRAhandle_tweets_10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1caaf4-ea9f-4875-b24a-7ca90fc4574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! du -h tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89349389-b1be-43bd-b8af-1071fc45bb7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! head tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e88a2e3-b054-4f0e-8018-1517974cf982",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i -e '1d' tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d9076-24ec-4609-b419-89f80c65bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 3 tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a293675-9397-4795-9f5f-69077536488d",
   "metadata": {},
   "source": [
    "### Задача - посчитать количество вхождений каждого слова в текстах сообщений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fe3b6-1b3e-4610-8b86-e3b42510e4e9",
   "metadata": {},
   "source": [
    "Попробуем написать наивное решение через Python + класс Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5270e-3b26-4f43-b746-62a662827182",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile simple_counter.py\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def wordcount_from_csv():\n",
    "    counter = Counter()\n",
    "    \n",
    "    with open('tweets.csv', 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            text = row[2].lower()\n",
    "            \n",
    "            words = re.findall(r'[a-z]+', text)\n",
    "            counter.update(words)\n",
    "    \n",
    "    return counter\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    c = wordcount_from_csv()\n",
    "    for key in c:\n",
    "        print(key, c[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a597a-43d8-437e-a519-6f904303238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! python3 simple_counter.py > counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558c3c0-24d0-4d55-b808-14a2db66fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580c3e6-f47f-40a4-b570-e8eaa8e763a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f8dba-826c-455c-9032-0b6406112035",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170aebca-83d4-4349-8990-28e5fb4f4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat counts.txt | sort -k2,2nr -k1,1 | head "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8bc992-271c-42d1-9a45-0b1d75a365c8",
   "metadata": {},
   "source": [
    "### А что если сделать mapreduce через Python и Bash?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62896ae-30f0-4928-bc63-fc30a5ad6f0a",
   "metadata": {},
   "source": [
    "**Map** — это первая стадия модели MapReduce, на которой входные данные разбиваются на независимые части и обрабатываются параллельно. Каждая часть поступает в функцию *map*, которая преобразует данные в набор промежуточных пар вида *(ключ, значение)*. Например, при подсчёте слов в тексте каждая строка преобразуется в список пар вроде `(\"слово\", 1)`. Эта стадия отвечает за извлечение и предварительную структуризацию информации.\n",
    "\n",
    "**Reduce** — это вторая стадия, которая получает сгруппированные по ключу результаты работы map-задач. Функция *reduce* сводит значения, относящиеся к одному ключу, к итоговому результату, например, суммируя, усредняя или объединяя их. В задаче подсчёта слов reduce просто складывает все единицы для каждого слова, чтобы получить общее количество его вхождений. Эта стадия отвечает за агрегацию и формирование конечного вывода.\n",
    "\n",
    "Интересно, что базовую идею MapReduce можно реализовать даже без Hadoop, используя обычные команды Bash. Поток данных можно пропустить через три этапа: `cat text.txt | map | sort | reduce > result.txt`. Здесь `map` — скрипт, который генерирует пары ключ–значение, `sort` выполняет роль *shuffle and sort*, а `reduce` агрегирует данные по ключам. Такой подход демонстрирует суть парадигмы MapReduce — разделение обработки на этапы отображения, сортировки и свёртки — без необходимости использовать распределённые вычисления.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216fb04d-72be-4a7b-aa81-e48955c6e4a4",
   "metadata": {},
   "source": [
    "### Напишем Python скрипт, который сможет выполнять map и reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94c325-c4ad-431d-bb2e-72334036b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wordcount.py\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def mapper():\n",
    "    reader = csv.reader(sys.stdin)\n",
    "    for row in reader:\n",
    "        text = row[2].lower() \n",
    "        words = re.findall(r'[a-z]+', text)\n",
    "        for word in words:\n",
    "            print(f\"{word}\\t1\")\n",
    "        \n",
    "\n",
    "def reducer():\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        word, count = line.strip().split('\\t', 1)\n",
    "        count = int(count)\n",
    "\n",
    "        if word == current_word or current_word is None:\n",
    "            current_count += count\n",
    "        else:\n",
    "            print(f\"{current_word}\\t{current_count}\")\n",
    "            current_count = 1\n",
    "            \n",
    "        current_word = word\n",
    "        \n",
    "    print(f\"{current_word}\\t{current_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117290fc-695f-411a-a286-85638d312b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374a9f5-a12d-4efa-86f7-f2829c7aaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! cat tweets.csv | python3 wordcount.py mapper | sort -k1,1 | python3 wordcount.py reducer > counts2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a4d1f-439f-4f79-8118-b5bfed50c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59771c7f-bf09-4eef-873e-60202006672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat counts2.txt | sort -k2,2nr -k1,1 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884d9a1-5174-4806-9cbf-2d8b2bd73306",
   "metadata": {},
   "source": [
    "### Напишем теперь аналогичный скрипт для подсчета top10 слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def3202-fb6d-4802-92b7-8398540d7df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile top10.py\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "def flush_stdin():\n",
    "    for line in sys.stdin:\n",
    "        pass\n",
    "        \n",
    "\n",
    "def mapper():\n",
    "    for line in sys.stdin:\n",
    "        print(line.strip() + '\\t')\n",
    "\n",
    "def reducer():\n",
    "    # читаем поток и выводим только первые 10 строк\n",
    "    for i, line in enumerate(sys.stdin):\n",
    "        if i < 10:\n",
    "            print(line.strip())\n",
    "        else:\n",
    "            break\n",
    "    flush_stdin()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8535f-5e5d-46ba-91e0-090d402829de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! cat counts2.txt | python3 top10.py mapper | sort -k2,2nr -k1,1 | python3 top10.py reducer > top10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17062f4-5670-44b5-a59c-1e415a50d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat top10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa3338-161b-42e5-ae3f-26a78a79cce6",
   "metadata": {},
   "source": [
    "### Так а что с Map Reduce?\n",
    "С помощью указанных двух скриптов был продемонстрирован основной принцип работы map reduce с помощью уже знакомых bash команд. Давайте теперь перейдем непосредственно к Hadoop MapReduce, ради которого здесь и собрались. Глобально, цель, которую он выполняет, это распределенный запуск скриптов Map & Reduce (на разных воркерах!) и сортировка данных между этими этапами (shuffle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ee338-29f1-4792-9da6-bd5fbd0673a7",
   "metadata": {},
   "source": [
    "**Переместим файлы с инпутами в hdfs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b905b-097b-46ae-995f-3079ac55c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /sem3\n",
    "! hdfs dfs -mkdir /sem3/wordcount\n",
    "! hdfs dfs -mkdir /sem3/wordcount/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e0641-aa93-4ab7-aefb-40ad0d4fea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -put ./tweets.csv /sem3/wordcount/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96cde2-1703-4b26-8aa6-2827ad0a994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -du -h /sem3/wordcount/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc511a-1b56-4bf4-b36c-ee9057f36d6b",
   "metadata": {},
   "source": [
    "**Запустим MapReduce таску**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247dae05-c218-4e6b-b5c7-4ee8416708b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.name=\"word_count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files wordcount.py \\\n",
    "-mapper \"python3 wordcount.py mapper\" \\\n",
    "-reducer \"python3 wordcount.py reducer\" \\\n",
    "-input /sem3/wordcount/input/ \\\n",
    "-output /sem3/wordcount/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfcb1f-7eb0-4d1f-8f74-86009fe2f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /sem3/wordcount/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29e979-598b-41f3-8fa2-ce4a184dcd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -rm /sem3/wordcount/output/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848ae68-9362-4870-93b3-337433e96f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/wordcount/output/part-* | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820eac7-db0a-4587-ad94-016b263d96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/wordcount/output/part-* > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a943fce-f575-480a-a5da-f27d0dbdaea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat result.txt | sort -k2,2nr -k1,1 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3626863-f4c3-414e-a526-cec9e6708900",
   "metadata": {},
   "source": [
    "**Давайте теперь запустим top10 скрипт через MapReduce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c56dde-b9ee-492f-b9b8-192e39ede0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /sem3/top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50de289-df46-4b15-91dc-25d4bfd896a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -rm -r -f /sem3/top10/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55384e72-bffe-4433-8858-8d844a36392a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.job.name=\"top10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.partition.keycomparator.options='-k2,2nr -k1,1' \\\n",
    "-files top10.py \\\n",
    "-mapper \"python3 top10.py mapper\" \\\n",
    "-reducer \"python3 top10.py reducer\" \\\n",
    "-input /sem3/wordcount/output/ \\\n",
    "-output /sem3/top10/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129837ae-a91a-4b03-92b1-4f96a2aa5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/top10/output/part-* 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51690c-f1ce-4922-b0c5-4b69a52fd298",
   "metadata": {},
   "source": [
    "### Distributed Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309a8d8-427d-4d21-a768-c5aa2d015b72",
   "metadata": {},
   "source": [
    "Кроме непосредственно **кода** в MapReduce мы также можем передавать на ноды-воркеры еще и другие файлы. Например, мы можем передать файл со словами, которые надо отфильтровать (топ 10 самых популярных слов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55bb8d-7c51-48a9-8a86-0ece5a511624",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ban.txt\n",
    "t\n",
    "co\n",
    "https\n",
    "to\n",
    "in\n",
    "s\n",
    "the\n",
    "news\n",
    "of\n",
    "world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d42e1-8bbb-475c-8aba-ca7b88e4a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile top10_ban.py\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "def flush_stdin():\n",
    "    for line in sys.stdin:\n",
    "        pass\n",
    "\n",
    "def get_banned_words():\n",
    "    with open('ban.txt', 'r') as f:\n",
    "        return {word.strip() for word in f}\n",
    "\n",
    "def mapper():\n",
    "    banned_words = get_banned_words()\n",
    "    for line in sys.stdin:\n",
    "        word, count = line.strip().split()\n",
    "        if word not in banned_words:\n",
    "            print(line.strip() + '\\t')\n",
    "\n",
    "def reducer():\n",
    "    # читаем поток и выводим только первые 10 строк\n",
    "    for i, line in enumerate(sys.stdin):\n",
    "        if i < 10:\n",
    "            print(line.strip())\n",
    "        else:\n",
    "            break\n",
    "    flush_stdin()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3d936-140a-40ff-8dbb-265aa7a69faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! cat counts2.txt | python3 top10_ban.py mapper | sort  -k2,2nr -k1,1 | python3 top10_ban.py reducer > top10_ban.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c265f66-42d2-4c4e-9258-6d55b93424ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat top10_ban.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e47a7-304e-4825-9c69-c0bf52e2b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /sem3/top10_ban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2bcac-136e-4306-ad51-019e42011b6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.job.name=\"top10_ban\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.partition.keycomparator.options='-k2,2nr -k1,1' \\\n",
    "-files top10_ban.py,ban.txt \\\n",
    "-mapper \"python3 top10_ban.py mapper\" \\\n",
    "-reducer \"python3 top10_ban.py reducer\" \\\n",
    "-input /sem3/wordcount/output/ \\\n",
    "-output /sem3/top10_ban/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722cf901-b3bf-426e-8714-d94a90710d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/top10_ban/output/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7115ee-a35b-4523-ad6a-3a4bd7271d31",
   "metadata": {},
   "source": [
    "### Combiner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8885549-5541-44ed-b719-b04370624399",
   "metadata": {},
   "source": [
    "![Combine](combiner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3174235-8dfd-4187-aee5-3de5675f84cc",
   "metadata": {},
   "source": [
    "**Combiner** — это вспомогательный мини-reducer, который выполняется на стороне mapper-узлов и служит для локального агрегирования промежуточных результатов перед отправкой их на этап shuffle. Он позволяет значительно сократить объём передаваемых данных в сеть: например, в задаче WordCount combiner может суммировать количество вхождений слов в пределах одного mapper’а, прежде чем эти частичные суммы будут отправлены на общий reducer. По сути, combiner использует ту же логику, что и reducer, но действует локально и не гарантируется к исполнению Hadoop’ом — это лишь оптимизация, которую фреймворк может применить, если посчитает возможным.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68211984-cf05-4976-a017-61d8cf44426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /sem3/wordcount_comb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070c056-27bb-462a-a1f8-75d3e35e253f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.name=\"word_count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files wordcount.py \\\n",
    "-mapper \"python3 wordcount.py mapper\" \\\n",
    "-reducer \"python3 wordcount.py reducer\" \\\n",
    "-combiner \"python3 wordcount.py reducer\" \\\n",
    "-input /sem3/wordcount/input/ \\\n",
    "-output /sem3/wordcount_comb/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf536299-21bf-4c64-b2de-c4a9e6654094",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/wordcount_comb/output/part-* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93503650-f2a9-4150-978e-590eb604ae30",
   "metadata": {},
   "source": [
    "### Custom Partitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fd22f-830d-496d-8bee-4b8ec9dadc9a",
   "metadata": {},
   "source": [
    "**Partitioner** — это компонент в MapReduce, который определяет, к какому reducer’у будет отправлен каждый ключ после стадии shuffle. Он отвечает за распределение промежуточных пар `key–value` между reduce-задачами, чтобы обеспечить баланс нагрузки и корректную группировку данных: все значения с одинаковым ключом должны попасть на один и тот же reducer. По умолчанию используется `HashPartitioner`, который распределяет ключи по хешу, но при необходимости можно задать **custom partitioner**, если нужно контролировать логику распределения — например, отправлять данные по диапазонам значений, по первым символам ключей или по географическим регионам.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45cdae-77bf-4a02-9853-a80adbce065a",
   "metadata": {},
   "source": [
    "Давайте решим следующую задачу: для каждого пользователя вычислим суммарную длину сообщений, написанных им на каждом языке. В данном случае стандартная сортировка по партициям работать не будет, вопрос - *почему?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb6662-bb17-4e9d-bae8-4223045ec253",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lang_len.py\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def mapper():\n",
    "    reader = csv.reader(sys.stdin)\n",
    "    for row in reader:\n",
    "        user, text, lang = row[1], row[2], row[4]\n",
    "        print(f'{user}+{lang}\\t{len(text)}')\n",
    "\n",
    "def reducer():\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        word, count = line.strip().split('\\t', 1)\n",
    "        count = int(count)\n",
    "\n",
    "        if word == current_word or current_word is None:\n",
    "            current_count += count\n",
    "        else:\n",
    "            print(f\"{current_word}\\t{current_count}\")\n",
    "            current_count = 1\n",
    "            \n",
    "        current_word = word\n",
    "        \n",
    "    print(f\"{current_word}\\t{current_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79664f97-3c95-4967-86d6-01da0ab4957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! cat tweets.csv | python3 lang_len.py mapper | sort | python3 lang_len.py reducer > lang_len.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c82f95-7e7b-48cc-b913-53862782974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sort -k1,1 lang_len.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3800f3-3572-46c9-bc16-de305881f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /sem3/lang_len/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4cccbe-2559-4b8b-9aaf-8ea1f990a1fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.name=\"lang_len\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D map.output.key.field.separator='+' \\\n",
    "-D mapreduce.partition.keypartitioner.options='-k1,1' \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options='-k1,1 -k2,2' \\\n",
    "-files lang_len.py \\\n",
    "-mapper \"python3 lang_len.py mapper\" \\\n",
    "-reducer \"python3 lang_len.py reducer\" \\\n",
    "-input /sem3/wordcount/input/ \\\n",
    "-output /sem3/lang_len/output/ \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6921b2ba-5770-42e3-8403-0e18b655f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /sem3/lang_len/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549daa33-7def-4ece-80d2-b86374b7cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/lang_len/output/part-00000 | sort -k1,1 | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21923f52-49d2-41ea-95db-e981ad80465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/lang_len/output/part-00002 | grep POLITOPROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130fb0c3-52b3-4837-9a3b-d0204e5607b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
