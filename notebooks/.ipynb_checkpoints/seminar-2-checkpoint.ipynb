{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005b1a8-53dd-42b9-bd81-af309121fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8415c246-2c35-41f6-80e3-578493f34775",
   "metadata": {},
   "source": [
    "### Что такое MapReduce\n",
    "\n",
    "**MapReduce** — это модель параллельных вычислений и фреймворк для обработки больших объёмов данных (Big Data).\n",
    "Она была предложена Google в статье *“MapReduce: Simplified Data Processing on Large Clusters”* (2004).\n",
    "Основная идея — разделить обработку данных на две стадии:\n",
    "\n",
    "1. **Map (отображение)** — применяет функцию ко всем входным данным и преобразует их в пары (ключ, значение).\n",
    "\n",
    "2. **Reduce (свёртка)** — агрегирует результаты по ключам, полученным на предыдущем шаге."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e54a2-bc7d-4aa7-a18f-38c34744bfc9",
   "metadata": {},
   "source": [
    "![MapReduce](mr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7bb06-e689-469b-bfe7-3b3e3ee68e6e",
   "metadata": {},
   "source": [
    "### Скачаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03c50d2-4d55-462a-9d9e-2b46a42cbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir seminar-2-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f62fb3-7e30-48fc-ad84-19ccfd2f54d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/seminar-2-dir\n"
     ]
    }
   ],
   "source": [
    "%cd seminar-2-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0d9b970-9ae5-4873-a0c0-b256a6a38fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  2524k      0  0:00:36  0:00:36 --:--:-- 3410k\n"
     ]
    }
   ],
   "source": [
    "! curl -o tweets.csv https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/refs/heads/master/IRAhandle_tweets_10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c1caaf4-ea9f-4875-b24a-7ca90fc4574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97M\ttweets.csv\n"
     ]
    }
   ],
   "source": [
    "! du -h tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89349389-b1be-43bd-b8af-1071fc45bb7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260338140,POLITICS_T0DAY,https://t.co/9OgJ5RxUEV,United States,Russian,2/16/2016 23:15,2/16/2016 23:16,92,887,12939,,Russian,0,NonEnglish,0,2260338140,699733931055259648,http://twitter.com/politics_t0day/statuses/699733931055259648,https://twitter.com/politics_t0day/status/699733931055259648/photo/1,,\n",
      "2260338140,POLITICS_T0DAY,Пять этажей жилого дома рухнули в Ярославле после взрыва газа https://t.co/nYwm9SVK9x с помощью @YouTube,United States,Russian,2/16/2016 5:41,2/16/2016 5:42,92,884,12895,,Russian,0,NonEnglish,0,2260338140,699468718888390656,http://twitter.com/politics_t0day/statuses/699468718888390656,https://youtu.be/STxTIceQmsA,,\n",
      "2260338140,POLITICS_T0DAY,Вербовщика Джихади Джона нашли в Турции через соцсети https://t.co/kJB3G1ioAm с помощью @YouTube,United States,Russian,2/16/2016 6:10,2/16/2016 6:10,92,884,12896,,Russian,0,NonEnglish,0,2260338140,699476018063659008,http://twitter.com/politics_t0day/statuses/699476018063659008,https://youtu.be/xy5ap3xX_fs,,\n",
      "2260338140,POLITICS_T0DAY,\"\"\"Война\"\" с Евгением Поддубным от 14.02.16 https://t.co/O0PXrduM9w с помощью @YouTube\",United States,Russian,2/16/2016 6:36,2/16/2016 6:36,92,885,12897,,Russian,0,NonEnglish,0,2260338140,699482457377210372,http://twitter.com/politics_t0day/statuses/699482457377210372,https://youtu.be/a7x4v7CYHiA,,\n",
      "2260338140,POLITICS_T0DAY,Посол #САР в #РФ обвинил #США в авиаударах по больнице «Врачей без границ» в Сирии.  https://t.co/FSTUolSqFU https://t.co/e7spZ9I67I,United States,Russian,2/16/2016 7:01,2/16/2016 7:01,92,885,12898,,Russian,0,NonEnglish,0,2260338140,699488793993330688,http://twitter.com/politics_t0day/statuses/699488793993330688,https://twitter.com/politics_t0day/status/699488793993330688/photo/1,https://vk.com/wall-62675857_176796,\n",
      "2260338140,POLITICS_T0DAY,В 2016 году начинается масштабная #модернизация оборонных предприятий #Крым'а.  https://t.co/GBR7A04eKQ #новости #РФ https://t.co/tjX132sC23,United States,Russian,2/16/2016 7:02,2/16/2016 7:02,92,885,12899,,Russian,0,NonEnglish,0,2260338140,699488942958239744,http://twitter.com/politics_t0day/statuses/699488942958239744,https://twitter.com/politics_t0day/status/699488942958239744/photo/1,https://vk.com/wall-62675857_176809,\n",
      "2260338140,POLITICS_T0DAY,Предательство #СССР. #Перестройка #Хрущёв'а.  https://t.co/mGKdv9xV6h https://t.co/nTSjJ4O1tD,United States,Russian,2/16/2016 7:02,2/16/2016 7:02,92,885,12900,,Russian,0,NonEnglish,0,2260338140,699489089805012992,http://twitter.com/politics_t0day/statuses/699489089805012992,https://twitter.com/politics_t0day/status/699489089805012992/photo/1,https://vk.com/wall-62675857_176845,\n",
      "2260338140,POLITICS_T0DAY,\"Телеканал \"\"#Россия\"\" покажет #фильм-расследование о коррупции в #США.  https://t.co/KiGMXoj3k4 #экономика #новости https://t.co/rT7CqWBVuk\",United States,Russian,2/16/2016 7:03,2/16/2016 7:03,92,885,12901,,Russian,0,NonEnglish,0,2260338140,699489262358700032,http://twitter.com/politics_t0day/statuses/699489262358700032,https://twitter.com/politics_t0day/status/699489262358700032/photo/1,https://vk.com/wall-62675857_176863,\n",
      "2260338140,POLITICS_T0DAY,#Поклонская вручила руководству #меджлис'а предписание о запрете деятельности  https://t.co/mes8iSqcVS #новости #Крым https://t.co/hFfAvDDMPq,United States,Russian,2/16/2016 7:04,2/16/2016 7:04,92,885,12903,,Russian,0,NonEnglish,0,2260338140,699489549815324674,http://twitter.com/politics_t0day/statuses/699489549815324674,https://twitter.com/politics_t0day/status/699489549815324674/photo/1,https://vk.com/wall-62675857_176892,\n",
      "2260338140,POLITICS_T0DAY,Обиженная #Турция может спровоцировать третью мировую.  https://t.co/gGe0kxpSS1 #новости #ИГИЛ #война #США #РФ https://t.co/o1JXLNMaSn,United States,Russian,2/16/2016 7:04,2/16/2016 7:04,92,885,12902,,Russian,0,NonEnglish,0,2260338140,699489393501933568,http://twitter.com/politics_t0day/statuses/699489393501933568,https://twitter.com/politics_t0day/status/699489393501933568/photo/1,https://vk.com/wall-62675857_176880,\n"
     ]
    }
   ],
   "source": [
    "! head tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e88a2e3-b054-4f0e-8018-1517974cf982",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i -e '1d' tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d3d9076-24ec-4609-b419-89f80c65bbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260338140,POLITICS_T0DAY,https://t.co/9OgJ5RxUEV,United States,Russian,2/16/2016 23:15,2/16/2016 23:16,92,887,12939,,Russian,0,NonEnglish,0,2260338140,699733931055259648,http://twitter.com/politics_t0day/statuses/699733931055259648,https://twitter.com/politics_t0day/status/699733931055259648/photo/1,,\n",
      "2260338140,POLITICS_T0DAY,Пять этажей жилого дома рухнули в Ярославле после взрыва газа https://t.co/nYwm9SVK9x с помощью @YouTube,United States,Russian,2/16/2016 5:41,2/16/2016 5:42,92,884,12895,,Russian,0,NonEnglish,0,2260338140,699468718888390656,http://twitter.com/politics_t0day/statuses/699468718888390656,https://youtu.be/STxTIceQmsA,,\n",
      "2260338140,POLITICS_T0DAY,Вербовщика Джихади Джона нашли в Турции через соцсети https://t.co/kJB3G1ioAm с помощью @YouTube,United States,Russian,2/16/2016 6:10,2/16/2016 6:10,92,884,12896,,Russian,0,NonEnglish,0,2260338140,699476018063659008,http://twitter.com/politics_t0day/statuses/699476018063659008,https://youtu.be/xy5ap3xX_fs,,\n"
     ]
    }
   ],
   "source": [
    "! head -n 3 tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a293675-9397-4795-9f5f-69077536488d",
   "metadata": {},
   "source": [
    "### Задача - посчитать количество вхождений каждого слова в текстах сообщений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fe3b6-1b3e-4610-8b86-e3b42510e4e9",
   "metadata": {},
   "source": [
    "Попробуем написать наивное решение через Python + класс Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52e5270e-3b26-4f43-b746-62a662827182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simple_counter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simple_counter.py\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def wordcount_from_csv():\n",
    "    counter = Counter()\n",
    "    \n",
    "    with open('tweets.csv', 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            text = row[2].lower()\n",
    "            \n",
    "            words = re.findall(r'[a-z]+', text)\n",
    "            counter.update(words)\n",
    "    \n",
    "    return counter\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    c = wordcount_from_csv()\n",
    "    for key in c:\n",
    "        print(key, c[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e5a597a-43d8-437e-a519-6f904303238a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.1 ms, sys: 18.5 ms, total: 37.6 ms\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! python3 simple_counter.py > counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d558c3c0-24d0-4d55-b808-14a2db66fb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts.txt  simple_counter.py  tweets.csv\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1580c3e6-f47f-40a4-b570-e8eaa8e763a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281837 counts.txt\n"
     ]
    }
   ],
   "source": [
    "! wc -l counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b2f8dba-826c-455c-9032-0b6406112035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https 172298\n",
      "t 211293\n",
      "co 200119\n",
      "ogj 4\n",
      "rxuev 1\n",
      "nywm 1\n",
      "svk 5\n",
      "x 3699\n",
      "youtube 2173\n",
      "kjb 4\n"
     ]
    }
   ],
   "source": [
    "! head counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "170aebca-83d4-4349-8990-28e5fb4f4347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t 211293\n",
      "co 200119\n",
      "https 172298\n",
      "to 51908\n",
      "in 44732\n",
      "s 39609\n",
      "the 36215\n",
      "news 34041\n",
      "of 30243\n",
      "world 29011\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "! cat counts.txt | sort -k2,2nr -k1,1 | head "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8bc992-271c-42d1-9a45-0b1d75a365c8",
   "metadata": {},
   "source": [
    "### А что если сделать mapreduce через Python и Bash?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62896ae-30f0-4928-bc63-fc30a5ad6f0a",
   "metadata": {},
   "source": [
    "**Map** — это первая стадия модели MapReduce, на которой входные данные разбиваются на независимые части и обрабатываются параллельно. Каждая часть поступает в функцию *map*, которая преобразует данные в набор промежуточных пар вида *(ключ, значение)*. Например, при подсчёте слов в тексте каждая строка преобразуется в список пар вроде `(\"слово\", 1)`. Эта стадия отвечает за извлечение и предварительную структуризацию информации.\n",
    "\n",
    "**Reduce** — это вторая стадия, которая получает сгруппированные по ключу результаты работы map-задач. Функция *reduce* сводит значения, относящиеся к одному ключу, к итоговому результату, например, суммируя, усредняя или объединяя их. В задаче подсчёта слов reduce просто складывает все единицы для каждого слова, чтобы получить общее количество его вхождений. Эта стадия отвечает за агрегацию и формирование конечного вывода.\n",
    "\n",
    "Интересно, что базовую идею MapReduce можно реализовать даже без Hadoop, используя обычные команды Bash. Поток данных можно пропустить через три этапа: `cat text.txt | map | sort | reduce > result.txt`. Здесь `map` — скрипт, который генерирует пары ключ–значение, `sort` выполняет роль *shuffle and sort*, а `reduce` агрегирует данные по ключам. Такой подход демонстрирует суть парадигмы MapReduce — разделение обработки на этапы отображения, сортировки и свёртки — без необходимости использовать распределённые вычисления.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216fb04d-72be-4a7b-aa81-e48955c6e4a4",
   "metadata": {},
   "source": [
    "### Напишем Python скрипт, который сможет выполнять map и reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1e94c325-c4ad-431d-bb2e-72334036b3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.py\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def mapper():\n",
    "    reader = csv.reader(sys.stdin)\n",
    "    for row in reader:\n",
    "        text = row[2].lower() \n",
    "        words = re.findall(r'[a-z]+', text)\n",
    "        for word in words:\n",
    "            print(f\"{word}\\t1\")\n",
    "        \n",
    "\n",
    "def reducer():\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        word, count = line.strip().split('\\t', 1)\n",
    "        count = int(count)\n",
    "\n",
    "        if word == current_word or current_word is None:\n",
    "            current_count += count\n",
    "        else:\n",
    "            print(f\"{current_word}\\t{current_count}\")\n",
    "            current_count = 1\n",
    "            \n",
    "        current_word = word\n",
    "        \n",
    "    print(f\"{current_word}\\t{current_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "117290fc-695f-411a-a286-85638d312b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts2.txt  counts.txt  simple_counter.py  tweets.csv\twordcount.py\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7374a9f5-a12d-4efa-86f7-f2829c7aaac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.7 ms, sys: 23.2 ms, total: 59.9 ms\n",
      "Wall time: 5.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! cat tweets.csv | python3 wordcount.py mapper | sort -k1,1 | python3 wordcount.py reducer > counts2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b36a4d1f-439f-4f79-8118-b5bfed50c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts2.txt    counts.txt  simple_counter.py  top10.txt   wordcount.py\n",
      "counts_mr.txt  result.txt  top10.py\t      tweets.csv\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "59771c7f-bf09-4eef-873e-60202006672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t25081\n",
      "aa\t131\n",
      "aaa\t26\n",
      "aaaaaa\t1\n",
      "aaaaaaaaassssss\t1\n",
      "aaaaaaaah\t1\n",
      "aaaaaand\t1\n",
      "aaaaand\t1\n",
      "aaaaannnddd\t1\n",
      "aaaaannnnndddd\t1\n",
      "aaaand\t1\n",
      "aaaapexeva\t1\n",
      "aaadqyw\t1\n",
      "aaaf\t2\n",
      "aaafdob\t1\n",
      "aaafzgea\t1\n",
      "aaags\t1\n",
      "aaah\t1\n",
      "aaahjkx\t1\n",
      "aaaihta\t1\n",
      "aaailtkoc\t1\n",
      "aaaisela\t1\n",
      "aaajdsl\t1\n",
      "aaalc\t1\n",
      "aaamcsfpzs\t1\n",
      "aaana\t1\n",
      "aaaniw\t2\n",
      "aaanmarkaz\t1\n",
      "aaaqtl\t1\n",
      "aaarzjhw\t1\n"
     ]
    }
   ],
   "source": [
    "! head -n 30 counts2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884d9a1-5174-4806-9cbf-2d8b2bd73306",
   "metadata": {},
   "source": [
    "### Напишем теперь аналогичный скрипт для подсчета top10 слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5def3202-fb6d-4802-92b7-8398540d7df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top10.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10.py\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "def flush_stdin():\n",
    "    for line in sys.stdin:\n",
    "        pass\n",
    "        \n",
    "\n",
    "def mapper():\n",
    "    for line in sys.stdin:\n",
    "        print(line.strip() + '\\t')\n",
    "\n",
    "def reducer():\n",
    "    # читаем поток и выводим только первые 10 строк\n",
    "    for i, line in enumerate(sys.stdin):\n",
    "        if i < 10:\n",
    "            print(line.strip())\n",
    "        else:\n",
    "            break\n",
    "    flush_stdin()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cdc8535f-5e5d-46ba-91e0-090d402829de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.3 ms, sys: 11.4 ms, total: 14.7 ms\n",
      "Wall time: 588 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! cat counts2.txt | python3 top10.py mapper | sort  -k2,2nr -k1,1 | python3 top10.py reducer > top10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a17062f4-5670-44b5-a59c-1e415a50d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t211293\n",
      "co\t200119\n",
      "https\t172298\n",
      "to\t51908\n",
      "in\t44732\n",
      "s\t39609\n",
      "the\t36215\n",
      "news\t34041\n",
      "of\t30243\n",
      "world\t29011\n"
     ]
    }
   ],
   "source": [
    "! cat top10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa3338-161b-42e5-ae3f-26a78a79cce6",
   "metadata": {},
   "source": [
    "### Так а что с Map Reduce?\n",
    "С помощью указанных двух скриптов был продемонстрирован основной принцип работы map reduce с помощью уже знакомых bash команд. Давайте теперь перейдем непосредственно к Hadoop MapReduce, ради которого здесь и собрались. Глобально, цель, которую он выполняет, это распределенный запуск скриптов Map & Reduce (на разных воркерах!) и сортировка данных между этими этапами (shuffle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ee338-29f1-4792-9da6-bd5fbd0673a7",
   "metadata": {},
   "source": [
    "**Переместим файлы с инпутами в hdfs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1b905b-097b-46ae-995f-3079ac55c793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 15:36:46,699 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 15:36:47,775 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 15:36:48,791 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -mkdir /sem3\n",
    "! hdfs dfs -mkdir /sem3/wordcount\n",
    "! hdfs dfs -mkdir /sem3/wordcount/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7e0641-aa93-4ab7-aefb-40ad0d4fea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 15:36:50,973 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put ./tweets.csv /sem3/wordcount/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c96cde2-1703-4b26-8aa6-2827ad0a994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-06 18:01:32,367 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "-rw-r--r--   1 hadoop users   94371615 2025-10-06 18:01 /sem3/wordcount/input/tweets.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /sem3/wordcount/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e5967a-10d8-4bb2-9d39-3ea333678af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-06 18:01:35,174 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -mkdir /sem3/wordcount/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d9019260-baa8-48ee-9b3c-19d400034327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-06 17:00:02,345 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Configured Capacity: 970947969024 (904.27 GB)\n",
      "Present Capacity: 843524689920 (785.59 GB)\n",
      "DFS Remaining: 843429511168 (785.50 GB)\n",
      "DFS Used: 95178752 (90.77 MB)\n",
      "DFS Used%: 0.01%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (2):\n",
      "\n",
      "Name: 172.18.0.4:9866 (map-reduce-infra-datanode1-1.map-reduce-infra_spark_mts)\n",
      "Hostname: 1322b44ff647\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 485473984512 (452.13 GB)\n",
      "DFS Used: 28672 (28 KB)\n",
      "Non DFS Used: 39023259648 (36.34 GB)\n",
      "DFS Remaining: 421714739200 (392.75 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 86.87%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Oct 06 17:00:01 UTC 2025\n",
      "Last Block Report: Mon Oct 06 10:35:24 UTC 2025\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n",
      "Name: 172.18.0.5:9866 (map-reduce-infra-datanode2-1.map-reduce-infra_spark_mts)\n",
      "Hostname: 24124ff99fe6\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 485473984512 (452.13 GB)\n",
      "DFS Used: 95150080 (90.74 MB)\n",
      "Non DFS Used: 38928105472 (36.25 GB)\n",
      "DFS Remaining: 421714771968 (392.75 GB)\n",
      "DFS Used%: 0.02%\n",
      "DFS Remaining%: 86.87%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Mon Oct 06 17:00:01 UTC 2025\n",
      "Last Block Report: Mon Oct 06 14:39:14 UTC 2025\n",
      "Num of Blocks: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc511a-1b56-4bf4-b36c-ee9057f36d6b",
   "metadata": {},
   "source": [
    "**Запустим MapReduce таску**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7379bb0d-6607-455b-84c9-c3570b4a8ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 16:41:16,797 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted /sem3/wordcount/output\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r -f /sem3/wordcount/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "247dae05-c218-4e6b-b5c7-4ee8416708b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:41:22,571 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar14582909534253861517/] [] /tmp/streamjob8303406821154922124.jar tmpDir=null\n",
      "2025-10-07 16:41:23,019 INFO  [main] client.DefaultNoHARMFailoverProxyProvider (DefaultNoHARMFailoverProxyProvider.java:init(64)) - Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2025-10-07 16:41:23,103 INFO  [main] client.DefaultNoHARMFailoverProxyProvider (DefaultNoHARMFailoverProxyProvider.java:init(64)) - Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2025-10-07 16:41:23,317 INFO  [main] mapreduce.JobResourceUploader (JobResourceUploader.java:disableErasureCodingForPath(907)) - Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1759851351637_0009\n",
      "2025-10-07 16:41:24,540 INFO  [main] mapred.FileInputFormat (FileInputFormat.java:listStatus(267)) - Total input files to process : 1\n",
      "2025-10-07 16:41:25,054 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(203)) - number of splits:2\n",
      "2025-10-07 16:41:25,588 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(299)) - Submitting tokens for job: job_1759851351637_0009\n",
      "2025-10-07 16:41:25,588 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(300)) - Executing with tokens: []\n",
      "2025-10-07 16:41:25,719 INFO  [main] conf.Configuration (Configuration.java:getConfResourceAsInputStream(2898)) - resource-types.xml not found\n",
      "2025-10-07 16:41:25,719 INFO  [main] resource.ResourceUtils (ResourceUtils.java:addResourcesFileToConf(476)) - Unable to find 'resource-types.xml'.\n",
      "2025-10-07 16:41:26,090 INFO  [main] impl.YarnClientImpl (YarnClientImpl.java:submitApplication(356)) - Submitted application application_1759851351637_0009\n",
      "2025-10-07 16:41:26,125 INFO  [main] mapreduce.Job (Job.java:submit(1681)) - The url to track the job: http://resourcemanager:8088/proxy/application_1759851351637_0009/\n",
      "2025-10-07 16:41:26,129 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1726)) - Running job: job_1759851351637_0009\n",
      "2025-10-07 16:41:35,352 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1747)) - Job job_1759851351637_0009 running in uber mode : false\n",
      "2025-10-07 16:41:35,357 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 0% reduce 0%\n",
      "2025-10-07 16:41:49,599 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 50% reduce 0%\n",
      "2025-10-07 16:41:50,639 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 0%\n",
      "2025-10-07 16:41:59,810 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 67%\n",
      "2025-10-07 16:42:00,840 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 100%\n",
      "2025-10-07 16:42:02,913 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1765)) - Job job_1759851351637_0009 completed successfully\n",
      "2025-10-07 16:42:03,194 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1772)) - Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=30312421\n",
      "\t\tFILE: Number of bytes written=62188426\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=94376022\n",
      "\t\tHDFS: Number of bytes written=2823611\n",
      "\t\tHDFS: Number of read operations=21\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22585\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=19872\n",
      "\t\tTotal time spent by all map tasks (ms)=22585\n",
      "\t\tTotal time spent by all reduce tasks (ms)=19872\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=22585\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=19872\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=23127040\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=20348928\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=257111\n",
      "\t\tMap output records=3195143\n",
      "\t\tMap output bytes=23922117\n",
      "\t\tMap output materialized bytes=30312439\n",
      "\t\tInput split bytes=198\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=281837\n",
      "\t\tReduce shuffle bytes=30312439\n",
      "\t\tReduce input records=3195143\n",
      "\t\tReduce output records=281837\n",
      "\t\tSpilled Records=6390286\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=497\n",
      "\t\tCPU time spent (ms)=26820\n",
      "\t\tPhysical memory (bytes) snapshot=3049385984\n",
      "\t\tVirtual memory (bytes) snapshot=17803907072\n",
      "\t\tTotal committed heap usage (bytes)=1188560896\n",
      "\t\tPeak Map Physical memory (bytes)=982175744\n",
      "\t\tPeak Map Virtual memory (bytes)=3931922432\n",
      "\t\tPeak Reduce Physical memory (bytes)=400191488\n",
      "\t\tPeak Reduce Virtual memory (bytes)=3339390976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=94375824\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2823611\n",
      "2025-10-07 16:42:03,194 INFO  [main] streaming.StreamJob (StreamJob.java:submitAndMonitorJob(1029)) - Output directory: /sem3/wordcount/output/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.name=\"word_count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files wordcount.py \\\n",
    "-mapper \"python3 wordcount.py mapper\" \\\n",
    "-reducer \"python3 wordcount.py reducer\" \\\n",
    "-input /sem3/wordcount/input/ \\\n",
    "-output /sem3/wordcount/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "04cfcb1f-7eb0-4d1f-8f74-86009fe2f9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 16:20:27,958 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 4 items\n",
      "-rw-r--r--   1 hadoop users          0 2025-10-07 16:20 /sem3/wordcount/output/_SUCCESS\n",
      "-rw-r--r--   1 hadoop users    3168124 2025-10-07 16:20 /sem3/wordcount/output/part-00000\n",
      "-rw-r--r--   1 hadoop users    3160856 2025-10-07 16:20 /sem3/wordcount/output/part-00001\n",
      "-rw-r--r--   1 hadoop users    3165548 2025-10-07 16:20 /sem3/wordcount/output/part-00002\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /sem3/wordcount/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c29e979-598b-41f3-8fa2-ce4a184dcd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 16:20:34,233 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted /sem3/wordcount/output/_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /sem3/wordcount/output/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38a09e14-7405-4c64-a8ee-3700834dabe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts2.txt  simple_counter.py\ttop10.txt   wordcount.py\n",
      "counts.txt   top10.py\t\ttweets.csv\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c848ae68-9362-4870-93b3-337433e96f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 16:20:39,141 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!\t1\n",
      "!!!жопы\t1\n",
      "!#lastminutegifts2016\t1\n",
      "!#snl\t1\n",
      "!)\t3\n",
      "!rt\t2\n",
      "!�@dj_mac_nificent�@elisiomark�@beatricelacy\t1\n",
      "\"\"\"though\t1\n",
      "\"\"(bye\t1\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /sem3/wordcount/output/part-* | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0820eac7-db0a-4587-ad94-016b263d96d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /sem3/wordcount/output/part-* > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a943fce-f575-480a-a5da-f27d0dbdaea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\t49804\n",
      "in\t42044\n",
      "в\t30617\n",
      "of\t29036\n",
      "the\t27454\n",
      "#world\t27070\n",
      "for\t22297\n",
      "#news,United\t22007\n",
      "a\t17592\n",
      "on\t17338\n",
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "! cat result.txt | sort -k2,2nr -k1,1 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3626863-f4c3-414e-a526-cec9e6708900",
   "metadata": {},
   "source": [
    "**Давайте теперь запустим top10 скрипт через MapReduce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57c56dde-b9ee-492f-b9b8-192e39ede0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 15:38:07,367 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -mkdir /sem3/top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e50de289-df46-4b15-91dc-25d4bfd896a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 16:46:53,193 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted /sem3/top10/output\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r -f /sem3/top10/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "55384e72-bffe-4433-8858-8d844a36392a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:46:55,320 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar11779773384336844669/] [] /tmp/streamjob12641805734492712068.jar tmpDir=null\n",
      "2025-10-07 16:46:55,721 INFO  [main] client.DefaultNoHARMFailoverProxyProvider (DefaultNoHARMFailoverProxyProvider.java:init(64)) - Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2025-10-07 16:46:55,808 INFO  [main] client.DefaultNoHARMFailoverProxyProvider (DefaultNoHARMFailoverProxyProvider.java:init(64)) - Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2025-10-07 16:46:56,011 INFO  [main] mapreduce.JobResourceUploader (JobResourceUploader.java:disableErasureCodingForPath(907)) - Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1759851351637_0010\n",
      "2025-10-07 16:46:56,759 INFO  [main] mapred.FileInputFormat (FileInputFormat.java:listStatus(267)) - Total input files to process : 3\n",
      "2025-10-07 16:46:57,335 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(203)) - number of splits:3\n",
      "2025-10-07 16:46:57,870 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(299)) - Submitting tokens for job: job_1759851351637_0010\n",
      "2025-10-07 16:46:57,870 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(300)) - Executing with tokens: []\n",
      "2025-10-07 16:46:58,009 INFO  [main] conf.Configuration (Configuration.java:getConfResourceAsInputStream(2898)) - resource-types.xml not found\n",
      "2025-10-07 16:46:58,009 INFO  [main] resource.ResourceUtils (ResourceUtils.java:addResourcesFileToConf(476)) - Unable to find 'resource-types.xml'.\n",
      "2025-10-07 16:46:58,324 INFO  [main] impl.YarnClientImpl (YarnClientImpl.java:submitApplication(356)) - Submitted application application_1759851351637_0010\n",
      "2025-10-07 16:46:58,357 INFO  [main] mapreduce.Job (Job.java:submit(1681)) - The url to track the job: http://resourcemanager:8088/proxy/application_1759851351637_0010/\n",
      "2025-10-07 16:46:58,358 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1726)) - Running job: job_1759851351637_0010\n",
      "2025-10-07 16:47:07,622 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1747)) - Job job_1759851351637_0010 running in uber mode : false\n",
      "2025-10-07 16:47:07,624 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 0% reduce 0%\n",
      "2025-10-07 16:47:17,883 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 67% reduce 0%\n",
      "2025-10-07 16:47:18,903 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 0%\n",
      "2025-10-07 16:47:25,981 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 100%\n",
      "2025-10-07 16:47:27,054 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1765)) - Job job_1759851351637_0010 completed successfully\n",
      "2025-10-07 16:47:27,294 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1772)) - Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3669128\n",
      "\t\tFILE: Number of bytes written=8591349\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2823911\n",
      "\t\tHDFS: Number of bytes written=100\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19625\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4974\n",
      "\t\tTotal time spent by all map tasks (ms)=19625\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4974\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=19625\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4974\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20096000\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5093376\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=281837\n",
      "\t\tMap output records=281837\n",
      "\t\tMap output bytes=3105448\n",
      "\t\tMap output materialized bytes=3669140\n",
      "\t\tInput split bytes=300\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=281837\n",
      "\t\tReduce shuffle bytes=3669140\n",
      "\t\tReduce input records=281837\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=563674\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=1058\n",
      "\t\tCPU time spent (ms)=13220\n",
      "\t\tPhysical memory (bytes) snapshot=2721435648\n",
      "\t\tVirtual memory (bytes) snapshot=13281882112\n",
      "\t\tTotal committed heap usage (bytes)=2004353024\n",
      "\t\tPeak Map Physical memory (bytes)=792494080\n",
      "\t\tPeak Map Virtual memory (bytes)=3325280256\n",
      "\t\tPeak Reduce Physical memory (bytes)=361132032\n",
      "\t\tPeak Reduce Virtual memory (bytes)=3306643456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2823611\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=100\n",
      "2025-10-07 16:47:27,295 INFO  [main] streaming.StreamJob (StreamJob.java:submitAndMonitorJob(1029)) - Output directory: /sem3/top10/output/\n",
      "CPU times: user 72.8 ms, sys: 12.9 ms, total: 85.6 ms\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.job.name=\"top10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.partition.keycomparator.options='-k2,2nr -k1,1' \\\n",
    "-files top10.py \\\n",
    "-mapper \"python3 top10.py mapper\" \\\n",
    "-reducer \"python3 top10.py reducer\" \\\n",
    "-input /sem3/wordcount/output/ \\\n",
    "-output /sem3/top10/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "129837ae-a91a-4b03-92b1-4f96a2aa5e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:47:28,006 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "t\t211293\n",
      "co\t200119\n",
      "https\t172298\n",
      "to\t51908\n",
      "in\t44732\n",
      "s\t39609\n",
      "the\t36215\n",
      "news\t34041\n",
      "of\t30243\n",
      "world\t29011\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /sem3/top10/output/part-* 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51690c-f1ce-4922-b0c5-4b69a52fd298",
   "metadata": {},
   "source": [
    "### Distributed Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309a8d8-427d-4d21-a768-c5aa2d015b72",
   "metadata": {},
   "source": [
    "Кроме непосредственно **кода** в MapReduce мы также можем передавать на ноды-воркеры еще и другие файлы. Например, мы можем передать файл со словами, которые надо отфильтровать (топ 10 самых популярных слов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1c55bb8d-7c51-48a9-8a86-0ece5a511624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ban.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ban.txt\n",
    "t\n",
    "co\n",
    "https\n",
    "to\n",
    "in\n",
    "s\n",
    "the\n",
    "news\n",
    "of\n",
    "world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7b7d42e1-8bbb-475c-8aba-ca7b88e4a606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top10_ban.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10_ban.py\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "def flush_stdin():\n",
    "    for line in sys.stdin:\n",
    "        pass\n",
    "\n",
    "def get_banned_words():\n",
    "    with open('ban.txt', 'r') as f:\n",
    "        return {word.strip() for word in f}\n",
    "\n",
    "def mapper():\n",
    "    banned_words = get_banned_words()\n",
    "    for line in sys.stdin:\n",
    "        word, count = line.strip().split()\n",
    "        if word not in banned_words:\n",
    "            print(line.strip() + '\\t')\n",
    "\n",
    "def reducer():\n",
    "    # читаем поток и выводим только первые 10 строк\n",
    "    for i, line in enumerate(sys.stdin):\n",
    "        if i < 10:\n",
    "            print(line.strip())\n",
    "        else:\n",
    "            break\n",
    "    flush_stdin()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "46e3d936-140a-40ff-8dbb-265aa7a69faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.52 ms, sys: 13.6 ms, total: 20.1 ms\n",
      "Wall time: 637 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! cat counts2.txt | python3 top10_ban.py mapper | sort  -k2,2nr -k1,1 | python3 top10_ban.py reducer > top10_ban.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0c265f66-42d2-4c4e-9258-6d55b93424ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http\t27528\n",
      "a\t25081\n",
      "for\t23601\n",
      "on\t18823\n",
      "i\t16211\n",
      "and\t15955\n",
      "u\t14703\n",
      "topnews\t14656\n",
      "is\t14448\n",
      "sports\t13213\n"
     ]
    }
   ],
   "source": [
    "cat top10_ban.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "148e47a7-304e-4825-9c69-c0bf52e2b489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 16:48:42,124 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -mkdir /sem3/top10_ban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9da2bcac-136e-4306-ad51-019e42011b6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:49:00,794 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar140181461329097395/] [] /tmp/streamjob17655520414586089727.jar tmpDir=null\n",
      "2025-10-07 16:49:01,212 INFO  [main] client.DefaultNoHARMFailoverProxyProvider (DefaultNoHARMFailoverProxyProvider.java:init(64)) - Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2025-10-07 16:49:01,298 INFO  [main] client.DefaultNoHARMFailoverProxyProvider (DefaultNoHARMFailoverProxyProvider.java:init(64)) - Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2025-10-07 16:49:01,480 INFO  [main] mapreduce.JobResourceUploader (JobResourceUploader.java:disableErasureCodingForPath(907)) - Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1759851351637_0011\n",
      "2025-10-07 16:49:02,672 INFO  [main] mapred.FileInputFormat (FileInputFormat.java:listStatus(267)) - Total input files to process : 3\n",
      "2025-10-07 16:49:03,216 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(203)) - number of splits:3\n",
      "2025-10-07 16:49:03,753 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(299)) - Submitting tokens for job: job_1759851351637_0011\n",
      "2025-10-07 16:49:03,753 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(300)) - Executing with tokens: []\n",
      "2025-10-07 16:49:03,913 INFO  [main] conf.Configuration (Configuration.java:getConfResourceAsInputStream(2898)) - resource-types.xml not found\n",
      "2025-10-07 16:49:03,913 INFO  [main] resource.ResourceUtils (ResourceUtils.java:addResourcesFileToConf(476)) - Unable to find 'resource-types.xml'.\n",
      "2025-10-07 16:49:03,993 INFO  [main] impl.YarnClientImpl (YarnClientImpl.java:submitApplication(356)) - Submitted application application_1759851351637_0011\n",
      "2025-10-07 16:49:04,016 INFO  [main] mapreduce.Job (Job.java:submit(1681)) - The url to track the job: http://resourcemanager:8088/proxy/application_1759851351637_0011/\n",
      "2025-10-07 16:49:04,017 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1726)) - Running job: job_1759851351637_0011\n",
      "2025-10-07 16:49:13,340 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1747)) - Job job_1759851351637_0011 running in uber mode : false\n",
      "2025-10-07 16:49:13,342 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 0% reduce 0%\n",
      "2025-10-07 16:49:23,652 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 33% reduce 0%\n",
      "2025-10-07 16:49:24,671 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 67% reduce 0%\n",
      "2025-10-07 16:49:25,680 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 0%\n",
      "2025-10-07 16:49:30,768 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 100%\n",
      "2025-10-07 16:49:32,853 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1765)) - Job job_1759851351637_0011 completed successfully\n",
      "2025-10-07 16:49:33,123 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1772)) - Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3668998\n",
      "\t\tFILE: Number of bytes written=8592313\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2823911\n",
      "\t\tHDFS: Number of bytes written=100\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19743\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5194\n",
      "\t\tTotal time spent by all map tasks (ms)=19743\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5194\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=19743\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5194\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20216832\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5318656\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=281837\n",
      "\t\tMap output records=281827\n",
      "\t\tMap output bytes=3105338\n",
      "\t\tMap output materialized bytes=3669010\n",
      "\t\tInput split bytes=300\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=281827\n",
      "\t\tReduce shuffle bytes=3669010\n",
      "\t\tReduce input records=281827\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=563654\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=1045\n",
      "\t\tCPU time spent (ms)=13660\n",
      "\t\tPhysical memory (bytes) snapshot=2715471872\n",
      "\t\tVirtual memory (bytes) snapshot=13287743488\n",
      "\t\tTotal committed heap usage (bytes)=2000683008\n",
      "\t\tPeak Map Physical memory (bytes)=796651520\n",
      "\t\tPeak Map Virtual memory (bytes)=3327655936\n",
      "\t\tPeak Reduce Physical memory (bytes)=371478528\n",
      "\t\tPeak Reduce Virtual memory (bytes)=3308724224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2823611\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=100\n",
      "2025-10-07 16:49:33,123 INFO  [main] streaming.StreamJob (StreamJob.java:submitAndMonitorJob(1029)) - Output directory: /sem3/top10_ban/output/\n",
      "CPU times: user 63 ms, sys: 19.5 ms, total: 82.6 ms\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.job.name=\"top10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.partition.keycomparator.options='-k2,2nr -k1,1' \\\n",
    "-files top10_ban.py,ban.txt \\\n",
    "-mapper \"python3 top10_ban.py mapper\" \\\n",
    "-reducer \"python3 top10_ban.py reducer\" \\\n",
    "-input /sem3/wordcount/output/ \\\n",
    "-output /sem3/top10_ban/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "722cf901-b3bf-426e-8714-d94a90710d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 16:49:56,334 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "http\t27528\n",
      "a\t25081\n",
      "for\t23601\n",
      "on\t18823\n",
      "i\t16211\n",
      "and\t15955\n",
      "u\t14703\n",
      "topnews\t14656\n",
      "is\t14448\n",
      "sports\t13213\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /sem3/top10_ban/output/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7115ee-a35b-4523-ad6a-3a4bd7271d31",
   "metadata": {},
   "source": [
    "### Combiner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8885549-5541-44ed-b719-b04370624399",
   "metadata": {},
   "source": [
    "![Combiner](combiner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3174235-8dfd-4187-aee5-3de5675f84cc",
   "metadata": {},
   "source": [
    "**Combiner** — это вспомогательный мини-reducer, который выполняется на стороне mapper-узлов и служит для локального агрегирования промежуточных результатов перед отправкой их на этап shuffle. Он позволяет значительно сократить объём передаваемых данных в сеть: например, в задаче WordCount combiner может суммировать количество вхождений слов в пределах одного mapper’а, прежде чем эти частичные суммы будут отправлены на общий reducer. По сути, combiner использует ту же логику, что и reducer, но действует локально и не гарантируется к исполнению Hadoop’ом — это лишь оптимизация, которую фреймворк может применить, если посчитает возможным.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "68211984-cf05-4976-a017-61d8cf44426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 16:54:27,341 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -mkdir /sem3/wordcount_comb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2070c056-27bb-462a-a1f8-75d3e35e253f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:55:59,409 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar8594863886566521833/] [] /tmp/streamjob15369750880871452449.jar tmpDir=null\n",
      "2025-10-07 16:55:59,791 INFO  [main] client.DefaultNoHARMFailoverProxyProvider (DefaultNoHARMFailoverProxyProvider.java:init(64)) - Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2025-10-07 16:55:59,872 INFO  [main] client.DefaultNoHARMFailoverProxyProvider (DefaultNoHARMFailoverProxyProvider.java:init(64)) - Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2025-10-07 16:56:00,054 INFO  [main] mapreduce.JobResourceUploader (JobResourceUploader.java:disableErasureCodingForPath(907)) - Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1759851351637_0013\n",
      "2025-10-07 16:56:00,721 INFO  [main] mapred.FileInputFormat (FileInputFormat.java:listStatus(267)) - Total input files to process : 1\n",
      "2025-10-07 16:56:01,253 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(203)) - number of splits:2\n",
      "2025-10-07 16:56:01,417 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(299)) - Submitting tokens for job: job_1759851351637_0013\n",
      "2025-10-07 16:56:01,417 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(300)) - Executing with tokens: []\n",
      "2025-10-07 16:56:01,594 INFO  [main] conf.Configuration (Configuration.java:getConfResourceAsInputStream(2898)) - resource-types.xml not found\n",
      "2025-10-07 16:56:01,594 INFO  [main] resource.ResourceUtils (ResourceUtils.java:addResourcesFileToConf(476)) - Unable to find 'resource-types.xml'.\n",
      "2025-10-07 16:56:01,920 INFO  [main] impl.YarnClientImpl (YarnClientImpl.java:submitApplication(356)) - Submitted application application_1759851351637_0013\n",
      "2025-10-07 16:56:01,980 INFO  [main] mapreduce.Job (Job.java:submit(1681)) - The url to track the job: http://resourcemanager:8088/proxy/application_1759851351637_0013/\n",
      "2025-10-07 16:56:01,981 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1726)) - Running job: job_1759851351637_0013\n",
      "2025-10-07 16:56:11,416 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1747)) - Job job_1759851351637_0013 running in uber mode : false\n",
      "2025-10-07 16:56:11,420 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 0% reduce 0%\n",
      "2025-10-07 16:56:28,861 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 0%\n",
      "2025-10-07 16:56:38,010 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 33%\n",
      "2025-10-07 16:56:39,024 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 67%\n",
      "2025-10-07 16:56:40,040 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 100%\n",
      "2025-10-07 16:56:42,184 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1765)) - Job job_1759851351637_0013 completed successfully\n",
      "2025-10-07 16:56:42,338 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1772)) - Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3843018\n",
      "\t\tFILE: Number of bytes written=9251530\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=94376022\n",
      "\t\tHDFS: Number of bytes written=2815991\n",
      "\t\tHDFS: Number of read operations=21\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=27515\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17511\n",
      "\t\tTotal time spent by all map tasks (ms)=27515\n",
      "\t\tTotal time spent by all reduce tasks (ms)=17511\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=27515\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=17511\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=28175360\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17931264\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=257111\n",
      "\t\tMap output records=3195143\n",
      "\t\tMap output bytes=23922117\n",
      "\t\tMap output materialized bytes=3843036\n",
      "\t\tInput split bytes=198\n",
      "\t\tCombine input records=3195143\n",
      "\t\tCombine output records=322538\n",
      "\t\tReduce input groups=281837\n",
      "\t\tReduce shuffle bytes=3843036\n",
      "\t\tReduce input records=322538\n",
      "\t\tReduce output records=281837\n",
      "\t\tSpilled Records=645076\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=1048\n",
      "\t\tCPU time spent (ms)=31770\n",
      "\t\tPhysical memory (bytes) snapshot=3594825728\n",
      "\t\tVirtual memory (bytes) snapshot=17889996800\n",
      "\t\tTotal committed heap usage (bytes)=1608515584\n",
      "\t\tPeak Map Physical memory (bytes)=1267576832\n",
      "\t\tPeak Map Virtual memory (bytes)=3973877760\n",
      "\t\tPeak Reduce Physical memory (bytes)=385048576\n",
      "\t\tPeak Reduce Virtual memory (bytes)=3334692864\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=94375824\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2815991\n",
      "2025-10-07 16:56:42,338 INFO  [main] streaming.StreamJob (StreamJob.java:submitAndMonitorJob(1029)) - Output directory: /sem3/wordcount_comb/output/\n",
      "CPU times: user 71.4 ms, sys: 19.8 ms, total: 91.2 ms\n",
      "Wall time: 43.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.name=\"word_count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files wordcount.py \\\n",
    "-mapper \"python3 wordcount.py mapper\" \\\n",
    "-reducer \"python3 wordcount.py reducer\" \\\n",
    "-combiner \"python3 wordcount.py reducer\" \\\n",
    "-input /sem3/wordcount/input/ \\\n",
    "-output /sem3/wordcount_comb/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cf536299-21bf-4c64-b2de-c4a9e6654094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 16:57:18,652 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "aa\t131\n",
      "aaaaaaaah\t1\n",
      "aaaaand\t1\n",
      "aaaand\t1\n",
      "aaags\t1\n",
      "aaah\t1\n",
      "aaahjkx\t1\n",
      "aaaihta\t1\n",
      "aaailtkoc\t1\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /sem3/wordcount_comb/output/part-* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93503650-f2a9-4150-978e-590eb604ae30",
   "metadata": {},
   "source": [
    "### Custom Partitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fd22f-830d-496d-8bee-4b8ec9dadc9a",
   "metadata": {},
   "source": [
    "**Partitioner** — это компонент в MapReduce, который определяет, к какому reducer’у будет отправлен каждый ключ после стадии shuffle. Он отвечает за распределение промежуточных пар `key–value` между reduce-задачами, чтобы обеспечить баланс нагрузки и корректную группировку данных: все значения с одинаковым ключом должны попасть на один и тот же reducer. По умолчанию используется `HashPartitioner`, который распределяет ключи по хешу, но при необходимости можно задать **custom partitioner**, если нужно контролировать логику распределения — например, отправлять данные по диапазонам значений, по первым символам ключей или по географическим регионам.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45cdae-77bf-4a02-9853-a80adbce065a",
   "metadata": {},
   "source": [
    "Давайте решим следующую задачу: для каждого пользователя вычислим суммарную длину сообщений, написанных им на каждом языке. В данном случае стандартная сортировка по партициям работать не будет, вопрос - *почему?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "23c0c1fd-0f96-40aa-86db-a826dc13ca5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "make an iterator that returns consecutive keys and groups from the iterable\n",
       "\n",
       "iterable\n",
       "  Elements to divide into groups according to the key function.\n",
       "key\n",
       "  A function for computing the group category for each element.\n",
       "  If the key function is not specified or is None, the element itself\n",
       "  is used for grouping.\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "?groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "48cb6662-bb17-4e9d-bae8-4223045ec253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lang_len.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lang_len.py\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def mapper():\n",
    "    reader = csv.reader(sys.stdin)\n",
    "    for row in reader:\n",
    "        user, text, lang = row[1], row[2], row[4]\n",
    "        print(f'{user}+{lang}\\t{len(text)}')\n",
    "\n",
    "def reducer():\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        word, count = line.strip().split('\\t', 1)\n",
    "        count = int(count)\n",
    "\n",
    "        if word == current_word or current_word is None:\n",
    "            current_count += count\n",
    "        else:\n",
    "            print(f\"{current_word}\\t{current_count}\")\n",
    "            current_count = 1\n",
    "            \n",
    "        current_word = word\n",
    "        \n",
    "    print(f\"{current_word}\\t{current_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "79664f97-3c95-4967-86d6-01da0ab4957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 ms, sys: 14.6 ms, total: 26.9 ms\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! cat tweets.csv | python3 lang_len.py mapper | sort | python3 lang_len.py reducer > lang_len.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "57c82f95-7e7b-48cc-b913-53862782974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCREAMYMONKEY+English\t3451894\n",
      "RIAFANRU+Russian\t2156506\n",
      "ROOMOFRUMOR+English\t1918287\n",
      "POLITICS_T0DAY+Russian\t1298626\n",
      "SANANTOTOPNEWS+English\t1006479\n",
      "ROBERTEBONYKING+English\t762336\n",
      "PRETTYLARAPLACE+English\t700711\n",
      "RH0LBR00K+English\t684761\n",
      "SEATTLE_POST+English\t596553\n",
      "SAMIRGOODEN+English\t593614\n"
     ]
    }
   ],
   "source": [
    "! sort -k2,2rn lang_len.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8b3800f3-3572-46c9-bc16-de305881f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 17:23:01,930 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -mkdir /sem3/lang_len/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3e4cccbe-2559-4b8b-9aaf-8ea1f990a1fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 17:30:35,923 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar714599146571878659/] [] /tmp/streamjob18318748059191830452.jar tmpDir=null\n",
      "2025-10-07 17:30:36,275 INFO  [main] client.DefaultNoHARMFailoverProxyProvider (DefaultNoHARMFailoverProxyProvider.java:init(64)) - Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2025-10-07 17:30:36,358 INFO  [main] client.DefaultNoHARMFailoverProxyProvider (DefaultNoHARMFailoverProxyProvider.java:init(64)) - Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "2025-10-07 17:30:36,526 INFO  [main] mapreduce.JobResourceUploader (JobResourceUploader.java:disableErasureCodingForPath(907)) - Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1759851351637_0015\n",
      "2025-10-07 17:30:36,804 INFO  [main] mapred.FileInputFormat (FileInputFormat.java:listStatus(267)) - Total input files to process : 1\n",
      "2025-10-07 17:30:37,294 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(203)) - number of splits:2\n",
      "2025-10-07 17:30:37,327 INFO  [main] Configuration.deprecation (Configuration.java:logDeprecation(1462)) - map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "2025-10-07 17:30:37,814 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(299)) - Submitting tokens for job: job_1759851351637_0015\n",
      "2025-10-07 17:30:37,814 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(300)) - Executing with tokens: []\n",
      "2025-10-07 17:30:37,958 INFO  [main] conf.Configuration (Configuration.java:getConfResourceAsInputStream(2898)) - resource-types.xml not found\n",
      "2025-10-07 17:30:37,958 INFO  [main] resource.ResourceUtils (ResourceUtils.java:addResourcesFileToConf(476)) - Unable to find 'resource-types.xml'.\n",
      "2025-10-07 17:30:38,039 INFO  [main] impl.YarnClientImpl (YarnClientImpl.java:submitApplication(356)) - Submitted application application_1759851351637_0015\n",
      "2025-10-07 17:30:38,060 INFO  [main] mapreduce.Job (Job.java:submit(1681)) - The url to track the job: http://resourcemanager:8088/proxy/application_1759851351637_0015/\n",
      "2025-10-07 17:30:38,061 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1726)) - Running job: job_1759851351637_0015\n",
      "2025-10-07 17:30:48,257 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1747)) - Job job_1759851351637_0015 running in uber mode : false\n",
      "2025-10-07 17:30:48,260 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 0% reduce 0%\n",
      "2025-10-07 17:30:58,490 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 0%\n",
      "2025-10-07 17:31:06,641 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 33%\n",
      "2025-10-07 17:31:07,656 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 67%\n",
      "2025-10-07 17:31:08,671 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1754)) -  map 100% reduce 100%\n",
      "2025-10-07 17:31:09,702 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1765)) - Job job_1759851351637_0015 completed successfully\n",
      "2025-10-07 17:31:09,980 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1772)) - Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7031536\n",
      "\t\tFILE: Number of bytes written=15632121\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=94376022\n",
      "\t\tHDFS: Number of bytes written=30742\n",
      "\t\tHDFS: Number of read operations=21\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13602\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15581\n",
      "\t\tTotal time spent by all map tasks (ms)=13602\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15581\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13602\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=15581\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13928448\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15954944\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=257111\n",
      "\t\tMap output records=257111\n",
      "\t\tMap output bytes=6517296\n",
      "\t\tMap output materialized bytes=7031554\n",
      "\t\tInput split bytes=198\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=22135\n",
      "\t\tReduce shuffle bytes=7031554\n",
      "\t\tReduce input records=257111\n",
      "\t\tReduce output records=1240\n",
      "\t\tSpilled Records=514222\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=875\n",
      "\t\tCPU time spent (ms)=14180\n",
      "\t\tPhysical memory (bytes) snapshot=2431426560\n",
      "\t\tVirtual memory (bytes) snapshot=16606760960\n",
      "\t\tTotal committed heap usage (bytes)=1594359808\n",
      "\t\tPeak Map Physical memory (bytes)=722706432\n",
      "\t\tPeak Map Virtual memory (bytes)=3346362368\n",
      "\t\tPeak Reduce Physical memory (bytes)=339890176\n",
      "\t\tPeak Reduce Virtual memory (bytes)=3306647552\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=94375824\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30742\n",
      "2025-10-07 17:31:09,981 INFO  [main] streaming.StreamJob (StreamJob.java:submitAndMonitorJob(1029)) - Output directory: /sem3/lang_len/output/\n",
      "CPU times: user 76.7 ms, sys: 17.7 ms, total: 94.4 ms\n",
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.name=\"lang_len\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D map.output.key.field.separator='+' \\\n",
    "-D mapreduce.partition.keypartitioner.options='-k1,1' \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options='-k1,1 -k2,2' \\\n",
    "-files lang_len.py \\\n",
    "-mapper \"python3 lang_len.py mapper\" \\\n",
    "-reducer \"python3 lang_len.py reducer\" \\\n",
    "-input /sem3/wordcount/input/ \\\n",
    "-output /sem3/lang_len/output/ \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6921b2ba-5770-42e3-8403-0e18b655f6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "2025-10-07 17:31:24,871 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 4 items\n",
      "-rw-r--r--   1 hadoop users          0 2025-10-07 17:31 /sem3/lang_len/output/_SUCCESS\n",
      "-rw-r--r--   1 hadoop users      10316 2025-10-07 17:31 /sem3/lang_len/output/part-00000\n",
      "-rw-r--r--   1 hadoop users      10431 2025-10-07 17:31 /sem3/lang_len/output/part-00001\n",
      "-rw-r--r--   1 hadoop users       9995 2025-10-07 17:31 /sem3/lang_len/output/part-00002\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /sem3/lang_len/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "549daa33-7def-4ece-80d2-b86374b7cf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "SCREAMYMONKEY+English\t3451894\n",
      "RIAFANRU+Russian\t2156506\n",
      "ROOMOFRUMOR+English\t1918287\n",
      "POLITICS_T0DAY+Russian\t1298626\n",
      "SANANTOTOPNEWS+English\t1006479\n",
      "ROBERTEBONYKING+English\t762336\n",
      "PRETTYLARAPLACE+English\t700711\n",
      "RH0LBR00K+English\t684761\n",
      "SEATTLE_POST+English\t596553\n",
      "SAMIRGOODEN+English\t593614\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /sem3/lang_len/output/part-* | sort -k2,2rn | head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
