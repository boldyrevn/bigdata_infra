{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ba4e18-8c16-4657-8b5c-2598921f46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e30b0d-fcc4-41e8-a7df-86b4382b06b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python3\n"
     ]
    }
   ],
   "source": [
    "! which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81111ad9-1af2-4a28-bc30-a4a2ce6af8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aarch64-conda_cos7-linux-gnu  cmake\t       conda-meta  lib\t    sbin   ssl\n",
      "aarch64-conda-linux-gnu       compiler_compat  etc\t   libexec  share\n",
      "bin\t\t\t      condabin\t       include\t   man\t    shell\n"
     ]
    }
   ],
   "source": [
    "! ls /opt/conda/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e4e110-8996-4067-9ab5-190866712f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestPySparkHDFS\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57db1690-1339-45fa-94d3-4da0af8a32f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TestPySparkHDFS</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff88115810>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f85101b-d296-43a0-af9f-70c928e817ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Локальный DataFrame ===\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o54.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (fa0273a53ac1 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1760991168829_0001/container_1760991168829_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 1104, in main\n    \"driver_version\": str(version),\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 6) than that in driver 3.11, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1760991168829_0001/container_1760991168829_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 1104, in main\n    \"driver_version\": str(version),\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 6) than that in driver 3.11, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Локальный DataFrame ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o54.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (fa0273a53ac1 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1760991168829_0001/container_1760991168829_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 1104, in main\n    \"driver_version\": str(version),\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 6) than that in driver 3.11, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1760991168829_0001/container_1760991168829_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 1104, in main\n    \"driver_version\": str(version),\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 6) than that in driver 3.11, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Alice\", 34),\n",
    "    (\"Bob\", 45),\n",
    "    (\"Charlie\", 28),\n",
    "    (\"Diana\", 31)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "print(\"\\n=== Локальный DataFrame ===\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c747a-6956-42a2-80d2-93462f2c44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    # .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    # .config(\"spark.yarn.resourcemanager.address\", \"resourcemanager:8032\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3bb18b-2eb3-4a01-bfc7-6ce026871e65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fec0dd-1f02-4fe5-b100-55671c001513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8415c246-2c35-41f6-80e3-578493f34775",
   "metadata": {},
   "source": [
    "### Что такое MapReduce\n",
    "\n",
    "**MapReduce** — это модель параллельных вычислений и фреймворк для обработки больших объёмов данных (Big Data).\n",
    "Она была предложена Google в статье *“MapReduce: Simplified Data Processing on Large Clusters”* (2004).\n",
    "Основная идея — разделить обработку данных на две стадии:\n",
    "\n",
    "1. **Map (отображение)** — применяет функцию ко всем входным данным и преобразует их в пары (ключ, значение).\n",
    "\n",
    "2. **Reduce (свёртка)** — агрегирует результаты по ключам, полученным на предыдущем шаге."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e54a2-bc7d-4aa7-a18f-38c34744bfc9",
   "metadata": {},
   "source": [
    "![MapReduce](mr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7bb06-e689-469b-bfe7-3b3e3ee68e6e",
   "metadata": {},
   "source": [
    "### Скачаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c50d2-4d55-462a-9d9e-2b46a42cbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir seminar-2-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f62fb3-7e30-48fc-ad84-19ccfd2f54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd seminar-2-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9b970-9ae5-4873-a0c0-b256a6a38fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -o tweets.csv https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/refs/heads/master/IRAhandle_tweets_10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1caaf4-ea9f-4875-b24a-7ca90fc4574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! du -h tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89349389-b1be-43bd-b8af-1071fc45bb7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! head tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e88a2e3-b054-4f0e-8018-1517974cf982",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i -e '1d' tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d9076-24ec-4609-b419-89f80c65bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 3 tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a293675-9397-4795-9f5f-69077536488d",
   "metadata": {},
   "source": [
    "### Задача - посчитать количество вхождений каждого слова в текстах сообщений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fe3b6-1b3e-4610-8b86-e3b42510e4e9",
   "metadata": {},
   "source": [
    "Попробуем написать наивное решение через Python + класс Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5270e-3b26-4f43-b746-62a662827182",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile simple_counter.py\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def wordcount_from_csv():\n",
    "    counter = Counter()\n",
    "    \n",
    "    with open('tweets.csv', 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            text = row[2].lower()\n",
    "            \n",
    "            words = re.findall(r'[a-z]+', text)\n",
    "            counter.update(words)\n",
    "    \n",
    "    return counter\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    c = wordcount_from_csv()\n",
    "    for key in c:\n",
    "        print(key, c[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a597a-43d8-437e-a519-6f904303238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! python3 simple_counter.py > counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558c3c0-24d0-4d55-b808-14a2db66fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580c3e6-f47f-40a4-b570-e8eaa8e763a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f8dba-826c-455c-9032-0b6406112035",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170aebca-83d4-4349-8990-28e5fb4f4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat counts.txt | sort -k2,2nr -k1,1 | head "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8bc992-271c-42d1-9a45-0b1d75a365c8",
   "metadata": {},
   "source": [
    "### А что если сделать mapreduce через Python и Bash?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62896ae-30f0-4928-bc63-fc30a5ad6f0a",
   "metadata": {},
   "source": [
    "**Map** — это первая стадия модели MapReduce, на которой входные данные разбиваются на независимые части и обрабатываются параллельно. Каждая часть поступает в функцию *map*, которая преобразует данные в набор промежуточных пар вида *(ключ, значение)*. Например, при подсчёте слов в тексте каждая строка преобразуется в список пар вроде `(\"слово\", 1)`. Эта стадия отвечает за извлечение и предварительную структуризацию информации.\n",
    "\n",
    "**Reduce** — это вторая стадия, которая получает сгруппированные по ключу результаты работы map-задач. Функция *reduce* сводит значения, относящиеся к одному ключу, к итоговому результату, например, суммируя, усредняя или объединяя их. В задаче подсчёта слов reduce просто складывает все единицы для каждого слова, чтобы получить общее количество его вхождений. Эта стадия отвечает за агрегацию и формирование конечного вывода.\n",
    "\n",
    "Интересно, что базовую идею MapReduce можно реализовать даже без Hadoop, используя обычные команды Bash. Поток данных можно пропустить через три этапа: `cat text.txt | map | sort | reduce > result.txt`. Здесь `map` — скрипт, который генерирует пары ключ–значение, `sort` выполняет роль *shuffle and sort*, а `reduce` агрегирует данные по ключам. Такой подход демонстрирует суть парадигмы MapReduce — разделение обработки на этапы отображения, сортировки и свёртки — без необходимости использовать распределённые вычисления.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216fb04d-72be-4a7b-aa81-e48955c6e4a4",
   "metadata": {},
   "source": [
    "### Напишем Python скрипт, который сможет выполнять map и reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94c325-c4ad-431d-bb2e-72334036b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wordcount.py\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def mapper():\n",
    "    reader = csv.reader(sys.stdin)\n",
    "    for row in reader:\n",
    "        text = row[2].lower() \n",
    "        words = re.findall(r'[a-z]+', text)\n",
    "        for word in words:\n",
    "            print(f\"{word}\\t1\")\n",
    "        \n",
    "\n",
    "def reducer():\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        word, count = line.strip().split('\\t', 1)\n",
    "        count = int(count)\n",
    "\n",
    "        if word == current_word or current_word is None:\n",
    "            current_count += count\n",
    "        else:\n",
    "            print(f\"{current_word}\\t{current_count}\")\n",
    "            current_count = 1\n",
    "            \n",
    "        current_word = word\n",
    "        \n",
    "    print(f\"{current_word}\\t{current_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117290fc-695f-411a-a286-85638d312b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374a9f5-a12d-4efa-86f7-f2829c7aaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! cat tweets.csv | python3 wordcount.py mapper | sort -k1,1 | python3 wordcount.py reducer > counts2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a4d1f-439f-4f79-8118-b5bfed50c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59771c7f-bf09-4eef-873e-60202006672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat counts2.txt | sort -k2,2nr -k1,1 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884d9a1-5174-4806-9cbf-2d8b2bd73306",
   "metadata": {},
   "source": [
    "### Напишем теперь аналогичный скрипт для подсчета top10 слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def3202-fb6d-4802-92b7-8398540d7df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile top10.py\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "def flush_stdin():\n",
    "    for line in sys.stdin:\n",
    "        pass\n",
    "        \n",
    "\n",
    "def mapper():\n",
    "    for line in sys.stdin:\n",
    "        print(line.strip() + '\\t')\n",
    "\n",
    "def reducer():\n",
    "    # читаем поток и выводим только первые 10 строк\n",
    "    for i, line in enumerate(sys.stdin):\n",
    "        if i < 10:\n",
    "            print(line.strip())\n",
    "        else:\n",
    "            break\n",
    "    flush_stdin()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8535f-5e5d-46ba-91e0-090d402829de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! cat counts2.txt | python3 top10.py mapper | sort -k2,2nr -k1,1 | python3 top10.py reducer > top10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17062f4-5670-44b5-a59c-1e415a50d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat top10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa3338-161b-42e5-ae3f-26a78a79cce6",
   "metadata": {},
   "source": [
    "### Так а что с Map Reduce?\n",
    "С помощью указанных двух скриптов был продемонстрирован основной принцип работы map reduce с помощью уже знакомых bash команд. Давайте теперь перейдем непосредственно к Hadoop MapReduce, ради которого здесь и собрались. Глобально, цель, которую он выполняет, это распределенный запуск скриптов Map & Reduce (на разных воркерах!) и сортировка данных между этими этапами (shuffle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ee338-29f1-4792-9da6-bd5fbd0673a7",
   "metadata": {},
   "source": [
    "**Переместим файлы с инпутами в hdfs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b905b-097b-46ae-995f-3079ac55c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /sem3\n",
    "! hdfs dfs -mkdir /sem3/wordcount\n",
    "! hdfs dfs -mkdir /sem3/wordcount/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e0641-aa93-4ab7-aefb-40ad0d4fea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -put ./tweets.csv /sem3/wordcount/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96cde2-1703-4b26-8aa6-2827ad0a994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -du -h /sem3/wordcount/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc511a-1b56-4bf4-b36c-ee9057f36d6b",
   "metadata": {},
   "source": [
    "**Запустим MapReduce таску**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247dae05-c218-4e6b-b5c7-4ee8416708b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.name=\"word_count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files wordcount.py \\\n",
    "-mapper \"python3 wordcount.py mapper\" \\\n",
    "-reducer \"python3 wordcount.py reducer\" \\\n",
    "-input /sem3/wordcount/input/ \\\n",
    "-output /sem3/wordcount/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfcb1f-7eb0-4d1f-8f74-86009fe2f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /sem3/wordcount/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29e979-598b-41f3-8fa2-ce4a184dcd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -rm /sem3/wordcount/output/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848ae68-9362-4870-93b3-337433e96f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/wordcount/output/part-* | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820eac7-db0a-4587-ad94-016b263d96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/wordcount/output/part-* > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a943fce-f575-480a-a5da-f27d0dbdaea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat result.txt | sort -k2,2nr -k1,1 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3626863-f4c3-414e-a526-cec9e6708900",
   "metadata": {},
   "source": [
    "**Давайте теперь запустим top10 скрипт через MapReduce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c56dde-b9ee-492f-b9b8-192e39ede0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /sem3/top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50de289-df46-4b15-91dc-25d4bfd896a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -rm -r -f /sem3/top10/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55384e72-bffe-4433-8858-8d844a36392a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.job.name=\"top10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.partition.keycomparator.options='-k2,2nr -k1,1' \\\n",
    "-files top10.py \\\n",
    "-mapper \"python3 top10.py mapper\" \\\n",
    "-reducer \"python3 top10.py reducer\" \\\n",
    "-input /sem3/wordcount/output/ \\\n",
    "-output /sem3/top10/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129837ae-a91a-4b03-92b1-4f96a2aa5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/top10/output/part-* 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51690c-f1ce-4922-b0c5-4b69a52fd298",
   "metadata": {},
   "source": [
    "### Distributed Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309a8d8-427d-4d21-a768-c5aa2d015b72",
   "metadata": {},
   "source": [
    "Кроме непосредственно **кода** в MapReduce мы также можем передавать на ноды-воркеры еще и другие файлы. Например, мы можем передать файл со словами, которые надо отфильтровать (топ 10 самых популярных слов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55bb8d-7c51-48a9-8a86-0ece5a511624",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ban.txt\n",
    "t\n",
    "co\n",
    "https\n",
    "to\n",
    "in\n",
    "s\n",
    "the\n",
    "news\n",
    "of\n",
    "world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d42e1-8bbb-475c-8aba-ca7b88e4a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile top10_ban.py\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "def flush_stdin():\n",
    "    for line in sys.stdin:\n",
    "        pass\n",
    "\n",
    "def get_banned_words():\n",
    "    with open('ban.txt', 'r') as f:\n",
    "        return {word.strip() for word in f}\n",
    "\n",
    "def mapper():\n",
    "    banned_words = get_banned_words()\n",
    "    for line in sys.stdin:\n",
    "        word, count = line.strip().split()\n",
    "        if word not in banned_words:\n",
    "            print(line.strip() + '\\t')\n",
    "\n",
    "def reducer():\n",
    "    # читаем поток и выводим только первые 10 строк\n",
    "    for i, line in enumerate(sys.stdin):\n",
    "        if i < 10:\n",
    "            print(line.strip())\n",
    "        else:\n",
    "            break\n",
    "    flush_stdin()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3d936-140a-40ff-8dbb-265aa7a69faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! cat counts2.txt | python3 top10_ban.py mapper | sort  -k2,2nr -k1,1 | python3 top10_ban.py reducer > top10_ban.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c265f66-42d2-4c4e-9258-6d55b93424ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat top10_ban.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e47a7-304e-4825-9c69-c0bf52e2b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /sem3/top10_ban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2bcac-136e-4306-ad51-019e42011b6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.job.name=\"top10_ban\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.partition.keycomparator.options='-k2,2nr -k1,1' \\\n",
    "-files top10_ban.py,ban.txt \\\n",
    "-mapper \"python3 top10_ban.py mapper\" \\\n",
    "-reducer \"python3 top10_ban.py reducer\" \\\n",
    "-input /sem3/wordcount/output/ \\\n",
    "-output /sem3/top10_ban/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722cf901-b3bf-426e-8714-d94a90710d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/top10_ban/output/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7115ee-a35b-4523-ad6a-3a4bd7271d31",
   "metadata": {},
   "source": [
    "### Combiner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8885549-5541-44ed-b719-b04370624399",
   "metadata": {},
   "source": [
    "![Combine](combiner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3174235-8dfd-4187-aee5-3de5675f84cc",
   "metadata": {},
   "source": [
    "**Combiner** — это вспомогательный мини-reducer, который выполняется на стороне mapper-узлов и служит для локального агрегирования промежуточных результатов перед отправкой их на этап shuffle. Он позволяет значительно сократить объём передаваемых данных в сеть: например, в задаче WordCount combiner может суммировать количество вхождений слов в пределах одного mapper’а, прежде чем эти частичные суммы будут отправлены на общий reducer. По сути, combiner использует ту же логику, что и reducer, но действует локально и не гарантируется к исполнению Hadoop’ом — это лишь оптимизация, которую фреймворк может применить, если посчитает возможным.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68211984-cf05-4976-a017-61d8cf44426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /sem3/wordcount_comb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070c056-27bb-462a-a1f8-75d3e35e253f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.name=\"word_count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files wordcount.py \\\n",
    "-mapper \"python3 wordcount.py mapper\" \\\n",
    "-reducer \"python3 wordcount.py reducer\" \\\n",
    "-combiner \"python3 wordcount.py reducer\" \\\n",
    "-input /sem3/wordcount/input/ \\\n",
    "-output /sem3/wordcount_comb/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf536299-21bf-4c64-b2de-c4a9e6654094",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/wordcount_comb/output/part-* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93503650-f2a9-4150-978e-590eb604ae30",
   "metadata": {},
   "source": [
    "### Custom Partitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fd22f-830d-496d-8bee-4b8ec9dadc9a",
   "metadata": {},
   "source": [
    "**Partitioner** — это компонент в MapReduce, который определяет, к какому reducer’у будет отправлен каждый ключ после стадии shuffle. Он отвечает за распределение промежуточных пар `key–value` между reduce-задачами, чтобы обеспечить баланс нагрузки и корректную группировку данных: все значения с одинаковым ключом должны попасть на один и тот же reducer. По умолчанию используется `HashPartitioner`, который распределяет ключи по хешу, но при необходимости можно задать **custom partitioner**, если нужно контролировать логику распределения — например, отправлять данные по диапазонам значений, по первым символам ключей или по географическим регионам.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45cdae-77bf-4a02-9853-a80adbce065a",
   "metadata": {},
   "source": [
    "Давайте решим следующую задачу: для каждого пользователя вычислим суммарную длину сообщений, написанных им на каждом языке. В данном случае стандартная сортировка по партициям работать не будет, вопрос - *почему?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb6662-bb17-4e9d-bae8-4223045ec253",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lang_len.py\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def mapper():\n",
    "    reader = csv.reader(sys.stdin)\n",
    "    for row in reader:\n",
    "        user, text, lang = row[1], row[2], row[4]\n",
    "        print(f'{user}+{lang}\\t{len(text)}')\n",
    "\n",
    "def reducer():\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        word, count = line.strip().split('\\t', 1)\n",
    "        count = int(count)\n",
    "\n",
    "        if word == current_word or current_word is None:\n",
    "            current_count += count\n",
    "        else:\n",
    "            print(f\"{current_word}\\t{current_count}\")\n",
    "            current_count = 1\n",
    "            \n",
    "        current_word = word\n",
    "        \n",
    "    print(f\"{current_word}\\t{current_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = sys.argv[1]\n",
    "    if mode == \"mapper\":\n",
    "        mapper()\n",
    "    elif mode == \"reducer\":\n",
    "        reducer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79664f97-3c95-4967-86d6-01da0ab4957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! cat tweets.csv | python3 lang_len.py mapper | sort | python3 lang_len.py reducer > lang_len.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c82f95-7e7b-48cc-b913-53862782974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sort -k1,1 lang_len.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3800f3-3572-46c9-bc16-de305881f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /sem3/lang_len/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4cccbe-2559-4b8b-9aaf-8ea1f990a1fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar \\\n",
    "-D mapreduce.job.name=\"lang_len\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D map.output.key.field.separator='+' \\\n",
    "-D mapreduce.partition.keypartitioner.options='-k1,1' \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options='-k1,1 -k2,2' \\\n",
    "-files lang_len.py \\\n",
    "-mapper \"python3 lang_len.py mapper\" \\\n",
    "-reducer \"python3 lang_len.py reducer\" \\\n",
    "-input /sem3/wordcount/input/ \\\n",
    "-output /sem3/lang_len/output/ \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6921b2ba-5770-42e3-8403-0e18b655f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /sem3/lang_len/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549daa33-7def-4ece-80d2-b86374b7cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/lang_len/output/part-00000 | sort -k1,1 | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21923f52-49d2-41ea-95db-e981ad80465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /sem3/lang_len/output/part-00002 | grep POLITOPROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130fb0c3-52b3-4837-9a3b-d0204e5607b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
