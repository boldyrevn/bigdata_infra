{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4292afe7",
   "metadata": {},
   "source": [
    "## команды запуска среды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e995de56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: mtspark.version\n",
      "Warning: Ignoring non-Spark config property: mtspark.python.interpreter\n",
      "Warning: Ignoring non-Spark config property: mtspark.python.path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: file = /data/home/iianishhe1/!WorkSpace/BNKSCOR-11043_check_feat_for_fs/python_3_9/lib/python3.9/site-packages/mtspark/config/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/07/15 09:54:16 WARN HiveConf: HiveConf of name hive.warehouse.data.skiptrash does not exist\n",
      "25/07/15 09:54:16 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "25/07/15 09:54:16 WARN Client: Exception encountered while connecting to the server \n",
      "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby\n",
      "\tat org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:376)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:623)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:414)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:843)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:839)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n",
      "\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n",
      "\tat com.sun.proxy.$Proxy30.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:674)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy31.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2507)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2483)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1485)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1482)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1499)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1474)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2388)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:750)\n",
      "\tat org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:507)\n",
      "\tat org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:982)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:220)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:222)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:585)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "25/07/15 09:54:16 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/07/15 09:54:21 WARN Client: Exception encountered while connecting to the server \n",
      "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby\n",
      "\tat org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:376)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:623)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:414)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:843)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:839)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n",
      "\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n",
      "\tat com.sun.proxy.$Proxy30.getFileInfo(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy31.getFileInfo(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)\n",
      "\tat org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:149)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.resolvePath(AbstractFileSystem.java:509)\n",
      "\tat org.apache.hadoop.fs.FileContext$25.next(FileContext.java:2305)\n",
      "\tat org.apache.hadoop.fs.FileContext$25.next(FileContext.java:2301)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.resolve(FileContext.java:2307)\n",
      "\tat org.apache.hadoop.fs.FileContext.resolvePath(FileContext.java:616)\n",
      "\tat org.apache.spark.deploy.yarn.Client.$anonfun$copyFileToRemote$5(Client.scala:477)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:475)\n",
      "\tat org.apache.spark.deploy.yarn.Client.distribute$1(Client.scala:556)\n",
      "\tat org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:673)\n",
      "\tat org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:982)\n",
      "\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:220)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:222)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:585)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "25/07/15 09:54:40 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "spark = ## старт вашей спарк сессии на машине"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bc1a9",
   "metadata": {},
   "source": [
    "## материал семинара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "153ec3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window ## пр-во методов под оконные ф-ии, но мы в семинаре их не рассмортим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db62bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## учебный пример фрема данных на семинар"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7141637b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://test-en-0015.msk.mts.ru:4203\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>seminar_4</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f78b9cbd1c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark ## смотрим на описание запущенной мной спарк сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da67267a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[continent: string, country: string, name: string, population: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## здесь создаем тестовый пример набора данных, с которым будем работать в рамках семинара\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "test_data = [\n",
    "{\"name\":\"Moscow\", \"country\":\"Russia\", \"continent\": \"Europe\", \"population\": 100_000_000},\n",
    "{ \"name\":\"Madrid\", \"country\":\"Spain\" },\n",
    "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 205_000_000},\n",
    "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 140_000_008},\n",
    "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" },\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 45_000_001 },\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 45_000_001 },\n",
    "]\n",
    "\n",
    "## создаем rdd\n",
    "rdd = sc.parallelize(test_data)\n",
    "\n",
    "## создаем DataFrame абстракцию поверх rdd\n",
    "df = spark.read.json(rdd).localCheckpoint()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f2c3c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/iianishhe1/DE_Sync_2023_2024\r\n"
     ]
    }
   ],
   "source": [
    "!pwd ## команда для получения пути до вашей home папки и места запуска ноутбука"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3659eb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwx------   - iianishhe1 hdfs          0 2025-07-03 03:00 /user/iianishhe1/.Trash\r\n",
      "drwxr-xr-x   - iianishhe1 hdfs          0 2025-07-03 16:00 /user/iianishhe1/.sparkStaging\r\n",
      "drwx------   - iianishhe1 hdfs          0 2024-07-19 18:25 /user/iianishhe1/.staging\r\n",
      "-rw-r--r--   3 iianishhe1 hdfs        232 2025-07-03 17:37 /user/iianishhe1/population.csv\r\n",
      "drwxr-xr-x   - iianishhe1 hdfs          0 2025-07-03 17:42 /user/iianishhe1/population_2.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h /user/iianishhe1/ ## здесь смотрим на путь до моей юзер папки на HDFS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fd93fa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[continent: string, country: string, name: string, population: double]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# чтение файла по пути папки под юзером на HDFS\n",
    "\n",
    "read_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .options(\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .load(\"/user/iianishhe1/population.csv\")\n",
    "\n",
    ")\n",
    "\n",
    "read_df <- # сформированный DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a9c8b40c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " continent  | Europe       \n",
      " country    | Russia       \n",
      " name       | Moscow       \n",
      " population | 1.0E8        \n",
      "-RECORD 1------------------\n",
      " continent  | null         \n",
      " country    | Spain        \n",
      " name       | Madrid       \n",
      " population | null         \n",
      "-RECORD 2------------------\n",
      " continent  | Europe       \n",
      " country    | France       \n",
      " name       | Paris        \n",
      " population | 2.05E8       \n",
      "-RECORD 3------------------\n",
      " continent  | Europe       \n",
      " country    | Germany      \n",
      " name       | Berlin       \n",
      " population | 1.40000008E8 \n",
      "-RECORD 4------------------\n",
      " continent  | Europe       \n",
      " country    | Spain        \n",
      " name       | Barselona    \n",
      " population | null         \n",
      "-RECORD 5------------------\n",
      " continent  | Africa       \n",
      " country    | Egypt        \n",
      " name       | Cairo        \n",
      " population | 4.5000001E7  \n",
      "-RECORD 6------------------\n",
      " continent  | Africa       \n",
      " country    | Egypt        \n",
      " name       | Cairo        \n",
      " population | 4.5000001E7  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_df.show(vertical=True) # вызываем action, смотрим данные в вертикальном разрезе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa272038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0x7fee6084e460>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#запись файла по пути до папки юзера на HDFS\n",
    "\n",
    "(\n",
    "    read_df\n",
    "    .write\n",
    "    .format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/user/iianishhe1/population_parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad6668f1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 iianishhe1 hdfs          0 2025-07-03 19:48 /user/iianishhe1/population_parquet/_SUCCESS\r\n",
      "-rw-r--r--   3 iianishhe1 hdfs      1.5 K 2025-07-03 19:48 /user/iianishhe1/population_parquet/part-00000-20b11fbc-7f3c-44a3-b6fd-c48abc1f6d6e-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h /user/iianishhe1/population_parquet/ # тут наблюдаем за тем как файл успешно записался"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb59f419",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAR1\u0015\u0004\u0015(\u0015,\u0015���\u000e<\u0015\u0004\u0015\u0004\u0000\u0000\u0014L\u0006\u0000\u0000\u0000Europe\u0006\u0000\u0000\u0000Africa\u0015\u0000\u0015\u0012\u0015\u0016\u0015����\u000b",
      "\u001c",
      "\u0015\u000e\u0015\u0004\u0015\u0006\u0015\b\u0000\u0000\t \u0002\u0000\u0000\u0000\u0003}\u0001\u00030\u0015\u0004\u0015b\u0015d\u0015�Ŭ\u0006<\u0015\r\n",
      "\u0015\u0004\u0000\u00001H\u0006\u0000\u0000\u0000Russia\u0005\u0000\u0000\u0000Spain\u0001\u0013dFrance\u0007\u0000\u0000\u0000Germany\u0005\u0000\u0000\u0000Egypt\u0015\u0000\u0015\u0016\u0015\u001a\u0015�ϐ�\u0006\u001c",
      "\u0015\u000e\u0015\u0004\u0015\u0006\u0015\b\u0000\u0000\u000b",
      "(\u0002\u0000\u0000\u0000\u0003\u0003\u0003�\u0016\u0012\u0015\u0004\u0015z\u0015x\u0015����\u0001<\u0015\f",
      "\u0015\u0004\u0000\u0000=$\u0006\u0000\u0000\u0000Moscow\u0005\r\n",
      "4adrid\u0005\u0000\u0000\u0000Paris\u0001\u0013lBerlin\t\u0000\u0000\u0000Barselona\u0005\u0000\u0000\u0000Cairo\u0015\u0000\u0015\u0016\u0015\u001a\u0015��ǜ\t\u001c",
      "\u0015\u000e\u0015\u0004\u0015\u0006\u0015\b\u0000\u0000\u000b",
      "(\u0002\u0000\u0000\u0000\u0003\u0003\u0003��\u0016\u0015\u0004\u0015@\u0015B\u0015����\u000e<\u0015\b\u0015\u0004\u0000\u0000 8\u0000\u0000\u0000\u0000�חA\u0000\u0000\u0000�\u001ap�\u0001\b0\u0010v��A\u0000\u0000\u0000\b*u�A\u0015\u0000\u0015\u0014\u0015\u0018\u0015�ʆ�\t\u001c",
      "\u0015\u000e\u0015\u0004\u0015\u0006\u0015\b\u0000\u0000\r\n",
      "$\u0002\u0000\u0000\u0000\u0003m\u0002\u0003�\u0003\u0019\u0011\u0002\u0019\u0018\u0006Africa\u0019\u0018\u0006Europe\u0015\u0002\u0019\u0016\u0002\u0000\u0019\u0011\u0002\u0019\u0018\u0005Egypt\u0019\u0018\u0005Spain\u0015\u0002\u0019\u0016\u0000\u0000\u0019\u0011\u0002\u0019\u0018\tBarselona\u0019\u0018\u0005Paris\u0015\u0002\u0019\u0016\u0000\u0000\u0019\u0011\u0002\u0019\u0018\b\u0000\u0000\u0000\b*u�A\u0019\u0018\b\u0000\u0000\u0000�\u001ap�A\u0015\u0002\u0019\u0016\u0004\u0000\u0019\u001c",
      "\u0016Z\u0015D\u0016\u0000\u0000\u0000\u0019\u001c",
      "\u0016�\u0002\u0015H\u0016\u0000\u0000\u0000\u0019\u001c",
      "\u0016�\u0004\u0015H\u0016\u0000\u0000\u0000\u0019\u001c",
      "\u0016�\u0005\u0015F\u0016\u0000\u0000\u0000\u0015\u0002\u0019\\H\f",
      "spark_schema\u0015\b\u0000\u0015\f",
      "%\u0002\u0018\tcontinent%\u0000L\u001c",
      "\u0000\u0000\u0000\u0015\f",
      "%\u0002\u0018\u0007country%\u0000L\u001c",
      "\u0000\u0000\u0000\u0015\f",
      "%\u0002\u0018\u0004name%\u0000L\u001c",
      "\u0000\u0000\u0000\u0015\r\n",
      "%\u0002\u0018\r\n",
      "population\u0000\u0016\u000e\u0019\u001c",
      "\u0019L&Z\u001c",
      "\u0015\f",
      "\u00195\b\u0006\u0004\u0019\u0018\tcontinent\u0015\u0002\u0016\u000e\u0016�\u0001\u0016�\u0001&Z&\b\u001c",
      "6\u0002(\u0006Europe\u0018\u0006Africa\u0000\u0019,\u0015\u0004\u0015\u0004\u0015\u0002\u0000\u0015\u0000\u0015\u0004\u0015\u0002\u0000\u0000\u0016�\u0007\u0015\u0014\u0016�\u0006\u00156\u0000&�\u0002\u001c",
      "\u0015\f",
      "\u00195\b\u0006\u0004\u0019\u0018\u0007country\u0015\u0002\u0016\u000e\u0016�\u0001\u0016�\u0001&�\u0002&�\u0001\u001c",
      "6\u0000(\u0005Spain\u0018\u0005Egypt\u0000\u0019,\u0015\u0004\u0015\u0004\u0015\u0002\u0000\u0015\u0000\u0015\u0004\u0015\u0002\u0000\u0000\u0016�\u0007\u0015\u0016\u0016�\u0006\u00152\u0000&�\u0004\u001c",
      "\u0015\f",
      "\u00195\b\u0006\u0004\u0019\u0018\u0004name\u0015\u0002\u0016\u000e\u0016�\u0001\u0016�\u0001&�\u0004&�\u0002\u001c",
      "6\u0000(\u0005Paris\u0018\tBarselona\u0000\u0019,\u0015\u0004\u0015\u0004\u0015\u0002\u0000\u0015\u0000\u0015\u0004\u0015\u0002\u0000\u0000\u0016�\b\u0015\u0016\u0016�\u0006\u0015:\u0000&�\u0005\u001c",
      "\u0015\r\n",
      "\u00195\b\u0006\u0004\u0019\u0018\r\n",
      "population\u0015\u0002\u0016\u000e\u0016�\u0001\u0016�\u0001&�\u0005&�\u0004\u001c",
      "\u0018\b\u0000\u0000\u0000�\u001ap�A\u0018\b\u0000\u0000\u0000\b*u�A\u0016\u0004(\b\u0000\u0000\u0000�\u001ap�A\u0018\b\u0000\u0000\u0000\b*u�A\u0000\u0019,\u0015\u0004\u0015\u0004\u0015\u0002\u0000\u0015\u0000\u0015\u0004\u0015\u0002\u0000\u0000\u0016�\b\u0015\u0016\u0016�\u0007\u0015>\u0000\u0016�\u0005\u0016\u000e&\b\u0016�\u0005\u0014\u0000\u0000\u0019,\u0018\u0018org.apache.spark.version\u0018\u00053.3.1\u0000\u0018)org.apache.spark.sql.parquet.row.metadata\u0018�\u0002{\"type\":\"struct\",\"fields\":[{\"name\":\"continent\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"country\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"population\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]}\u0000\u0018Jparquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\u0019L\u001c",
      "\u0000\u0000\u001c",
      "\u0000\u0000\u001c",
      "\u0000\u0000\u001c",
      "\u0000\u0000\u0000�\u0003\u0000\u0000PAR1"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/iianishhe1/population_parquet/* # тут попытались вывести файл на чтение в консоль\n",
    "# но формат паркет хранит данные бинарно, так что просто так их не прочитать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "14afc5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>continent</th>\n",
       "      <th>country</th>\n",
       "      <th>name</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>100000000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Europe</td>\n",
       "      <td>France</td>\n",
       "      <td>Paris</td>\n",
       "      <td>205000000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>140000008.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barselona</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Africa</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>45000001.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Africa</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>45000001.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  continent  country       name    population\n",
       "0    Europe   Russia     Moscow 100000000.000\n",
       "1       NaN    Spain     Madrid           NaN\n",
       "2    Europe   France      Paris 205000000.000\n",
       "3    Europe  Germany     Berlin 140000008.000\n",
       "4    Europe    Spain  Barselona           NaN\n",
       "5    Africa    Egypt      Cairo  45000001.000\n",
       "6    Africa    Egypt      Cairo  45000001.000"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Создание DataFrame от локального файла, он лежит рядом с ноутбуком, не на HDFS!!!\n",
    "pdf = pd.read_csv(\"population.csv\")\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b4719bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[continent: string, country: string, name: string, population: double]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pdf = (\n",
    "    spark.createDataFrame(pdf) ## создание DataFrame от локально pandas объекта\n",
    ")\n",
    "\n",
    "test_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "71860d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>continent</th>\n",
       "      <th>country</th>\n",
       "      <th>name</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>100000000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Europe</td>\n",
       "      <td>France</td>\n",
       "      <td>Paris</td>\n",
       "      <td>205000000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>140000008.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barselona</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Africa</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>45000001.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Africa</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>45000001.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  continent  country       name    population\n",
       "0    Europe   Russia     Moscow 100000000.000\n",
       "1      None    Spain     Madrid           NaN\n",
       "2    Europe   France      Paris 205000000.000\n",
       "3    Europe  Germany     Berlin 140000008.000\n",
       "4    Europe    Spain  Barselona           NaN\n",
       "5    Africa    Egypt      Cairo  45000001.000\n",
       "6    Africa    Egypt      Cairo  45000001.000"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pdf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e0d1c",
   "metadata": {},
   "source": [
    "Смотрим на методы взаимодействия с DataFrame:\n",
    "- select\n",
    "- filter\n",
    "- dropDuplicates\n",
    "- na.fill\n",
    "- replace\n",
    "- orderBy\n",
    "\n",
    "Триггерные действия:\n",
    "- show\n",
    "- save\n",
    "- saveAsTable - просто рассказать\n",
    "\n",
    "документация со всем, что может пригодиться:\n",
    "- методы к DataFrame - https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\n",
    "- методы из sql.functions - https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n",
    "\n",
    "- Посмотрим .explain() и на сами планы\n",
    "- alias и когда он нужен\n",
    "- join и как выглядят их планы вычислений\n",
    "\n",
    "Будем переодически смотреть на UI и как там все это выглядит\n",
    "\n",
    "Полезная ссылка - https://habr.com/ru/articles/901078/ - тут написано про Catalyst и как он оптимизирует ваши запросы к данным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Важно добавить, в sql.functions методах для указания колонок, над которыми данные хотите применить методы\n",
    "## указывать колонки через перечисление string переменных, либо через метод F.col() - к которому также можно применять свои методы\n",
    "## Второй подход более предпочтительный, так как к объектам колонок F.col() можно применить дополнительные методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b4950327",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter ('a.country = Russia)\n",
      "   +- 'Join Inner, ('a.name = 'b.name)\n",
      "      :- 'SubqueryAlias a\n",
      "      :  +- 'UnresolvedRelation [test_test], [], false\n",
      "      +- 'SubqueryAlias b\n",
      "         +- 'UnresolvedRelation [test_test], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint, continent: string, country: string, name: string, population: bigint\n",
      "Project [continent#189, country#190, name#191, population#192L, continent#2473, country#2474, name#2475, population#2476L]\n",
      "+- Filter (country#190 = Russia)\n",
      "   +- Join Inner, (name#191 = name#2475)\n",
      "      :- SubqueryAlias a\n",
      "      :  +- SubqueryAlias test_test\n",
      "      :     +- View (`test_test`, [continent#189,country#190,name#191,population#192L])\n",
      "      :        +- LogicalRDD [continent#189, country#190, name#191, population#192L], false\n",
      "      +- SubqueryAlias b\n",
      "         +- SubqueryAlias test_test\n",
      "            +- View (`test_test`, [continent#2473,country#2474,name#2475,population#2476L])\n",
      "               +- LogicalRDD [continent#2473, country#2474, name#2475, population#2476L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (name#191 = name#2475)\n",
      ":- Filter ((isnotnull(country#190) AND (country#190 = Russia)) AND isnotnull(name#191))\n",
      ":  +- LogicalRDD [continent#189, country#190, name#191, population#192L], false\n",
      "+- Filter isnotnull(name#2475)\n",
      "   +- LogicalRDD [continent#2473, country#2474, name#2475, population#2476L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [name#191], [name#2475], Inner\n",
      "   :- Sort [name#191 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(name#191, 400), ENSURE_REQUIREMENTS, [plan_id=3334]\n",
      "   :     +- Filter ((isnotnull(country#190) AND (country#190 = Russia)) AND isnotnull(name#191))\n",
      "   :        +- Scan ExistingRDD[continent#189,country#190,name#191,population#192L]\n",
      "   +- Sort [name#2475 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(name#2475, 400), ENSURE_REQUIREMENTS, [plan_id=3335]\n",
      "         +- Filter isnotnull(name#2475)\n",
      "            +- Scan ExistingRDD[continent#2473,country#2474,name#2475,population#2476L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Посмотреть как выглядит реализация какой-то задачи на SQL\n",
    "\n",
    "df.createOrReplaceTempView(\"test_test\") ## создание метки view для обращения к ней через \"FROM test_test\" \n",
    "\n",
    "\n",
    "## Далее всем знакомый вам SQL код\n",
    "spark.sql(\"\"\"\n",
    "    select *\n",
    "    \n",
    "    from test_test as a join test_test as b on a.name = b.name\n",
    "    \n",
    "    where a.name = 'Moscow'\n",
    "\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c88b4b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+----------+---------+-------+----------+\n",
      "|  name|continent|country|population|continent|country|population|\n",
      "+------+---------+-------+----------+---------+-------+----------+\n",
      "|Moscow|   Europe| Russia| 100000000|   Europe| Russia| 100000000|\n",
      "+------+---------+-------+----------+---------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Реализация запроса выше через DataFrame методы\n",
    "\n",
    "(\n",
    "    df\n",
    "    .select(\"*\")\n",
    "    .join(\n",
    "        df,\n",
    "        on=[\"name\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .filter(F.col(\"name\") == \"Moscow\")\n",
    "    .show()\n",
    ")\n",
    "\n",
    "## реализовали .select(\"*\") - всех колонок, но так делать не стоит!!! явно указывайте список нужных колонок\n",
    "## .join() - джоин первого датафрейма ко второму по нужной колонке и с условием inner\n",
    "## добавление фильтра по колонке name с опр значениями в ней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1fa714c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "|     name|continent|country|\n",
      "+---------+---------+-------+\n",
      "|   Moscow|   Europe| Russia|\n",
      "|   Madrid|     null|  Spain|\n",
      "|    Paris|   Europe| France|\n",
      "|   Berlin|   Europe|Germany|\n",
      "|Barselona|   Europe|  Spain|\n",
      "|    Cairo|   Africa|  Egypt|\n",
      "|    Cairo|   Africa|  Egypt|\n",
      "+---------+---------+-------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Project ['name, 'continent, 'country]\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "name: string, continent: string, country: string\n",
      "Project [name#8, continent#6, country#7]\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [name#8, continent#6, country#7]\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [name#8, continent#6, country#7]\n",
      "+- *(1) Scan ExistingRDD[continent#6,country#7,name#8,population#9L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## разбираем отдельно методы и их планы запросов:\n",
    "    \n",
    "(\n",
    "    df\n",
    "    .select(\"name\", \"continent\", \"country\") ## метод выбора нужных колонок из дата фрейма\n",
    "    .show()\n",
    ")\n",
    "\n",
    "(\n",
    "    df\n",
    "    .select(\"name\", \"continent\", \"country\")\n",
    "    .explain(True)\n",
    ")\n",
    "\n",
    "## по плану запроса видно, что Physical Plan (физический движок с вызовом конкретных методов)\n",
    "## сначала делает скан  коллекции а после вызывает метод Project для выделения нужных колонок из данных\n",
    "## ПЛАНЫ ЗАПРОСОВ ЧИТАЮТСЯ СНИЗУ ВВЕРХ\n",
    "\n",
    "## отличия каждого из планов более подробно разобраны на лекции, в блоке про их чтение и определение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f924da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+----------+\n",
      "|continent|country|  name|population|\n",
      "+---------+-------+------+----------+\n",
      "|   Europe| Russia|Moscow| 100000000|\n",
      "+---------+-------+------+----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Filter ('name = Moscow)\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint\n",
      "Filter (name#8 = Moscow)\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(name#8) AND (name#8 = Moscow))\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(name#8) AND (name#8 = Moscow))\n",
      "+- *(1) Scan ExistingRDD[continent#6,country#7,name#8,population#9L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .filter(F.col(\"name\") == \"Moscow\") ## условие фильтрации данных по значению из колонки, аналог WHERE в SQL\n",
    "    .show()\n",
    ")\n",
    "\n",
    "(\n",
    "    df\n",
    "    .filter(F.col(\"name\") == \"Moscow\")\n",
    "    .explain(True)\n",
    ")\n",
    "\n",
    "## по плану запроса видно, что Physical Plan (физический движок с вызовом конкретных методов)\n",
    "## сначала делает скан  коллекции а после применяется метод Filter для проверки того условия,\n",
    "## что мы указали в методах к DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2c69d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe| Russia|   Moscow| 100000000|\n",
      "|     null|  Spain|   Madrid|      null|\n",
      "|   Europe| France|    Paris| 205000000|\n",
      "|   Europe|Germany|   Berlin| 140000008|\n",
      "|   Europe|  Spain|Barselona|      null|\n",
      "|   Africa|  Egypt|    Cairo|  45000001|\n",
      "|   Africa|  Egypt|    Cairo|  45000001|\n",
      "+---------+-------+---------+----------+\n",
      "\n",
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe| Russia|   Moscow| 100000000|\n",
      "|   Europe| France|    Paris| 205000000|\n",
      "|   Europe|Germany|   Berlin| 140000008|\n",
      "|   Europe|  Spain|Barselona|      null|\n",
      "|   Africa|  Egypt|    Cairo|  45000001|\n",
      "|     null|  Spain|   Madrid|      null|\n",
      "+---------+-------+---------+----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Deduplicate [continent#6, country#7, name#8, population#9L]\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint\n",
      "Deduplicate [continent#6, country#7, name#8, population#9L]\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [continent#6, country#7, name#8, population#9L], [continent#6, country#7, name#8, population#9L]\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[continent#6, country#7, name#8, population#9L], functions=[], output=[continent#6, country#7, name#8, population#9L])\n",
      "   +- Exchange hashpartitioning(continent#6, country#7, name#8, population#9L, 400), ENSURE_REQUIREMENTS, [plan_id=381]\n",
      "      +- HashAggregate(keys=[continent#6, country#7, name#8, population#9L], functions=[], output=[continent#6, country#7, name#8, population#9L])\n",
      "         +- Scan ExistingRDD[continent#6,country#7,name#8,population#9L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .show() ## в таком вызове в данных видим полные дубли по строкам, конкретно в данных по Африке\n",
    ")\n",
    "\n",
    "(\n",
    "    df\n",
    "    .dropDuplicates() ## метод выявления дублей и их удалние, если аргументов нет - дубли ищутся по всем строкам\n",
    "    .show()\n",
    ")\n",
    "\n",
    "(\n",
    "    df\n",
    "    .dropDuplicates() \n",
    "    .explain(True)\n",
    ")\n",
    "\n",
    "## по плану запроса видно, что Physical Plan (физический движок с вызовом конкретных методов)\n",
    "## сначала скан коллекции а после связка трех методов - HashAggregate, Exchange hashpartitioning и снова HashAggregate\n",
    "## посянение почему так метод реализуется на уровне движка лучше в записи семинара\n",
    "## я там явно проговариваю логику работы оптимизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ca737f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe| Russia|   Moscow| 100000000|\n",
      "|     null|  Spain|   Madrid|      null|\n",
      "|   Europe| France|    Paris| 205000000|\n",
      "|   Europe|Germany|   Berlin| 140000008|\n",
      "|   Europe|  Spain|Barselona|      null|\n",
      "|   Africa|  Egypt|    Cairo|  45000001|\n",
      "|   Africa|  Egypt|    Cairo|  45000001|\n",
      "+---------+-------+---------+----------+\n",
      "\n",
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|     null|  Spain|   Madrid|      null|\n",
      "|   Africa|  Egypt|    Cairo|  45000001|\n",
      "|   Africa|  Egypt|    Cairo|  45000001|\n",
      "|   Europe| Russia|   Moscow| 100000000|\n",
      "|   Europe|Germany|   Berlin| 140000008|\n",
      "|   Europe| France|    Paris| 205000000|\n",
      "|   Europe|  Spain|Barselona|      null|\n",
      "+---------+-------+---------+----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['continent ASC NULLS FIRST], true\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint\n",
      "Sort [continent#6 ASC NULLS FIRST], true\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [continent#6 ASC NULLS FIRST], true\n",
      "+- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [continent#6 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(continent#6 ASC NULLS FIRST, 400), ENSURE_REQUIREMENTS, [plan_id=411]\n",
      "      +- Scan ExistingRDD[continent#6,country#7,name#8,population#9L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .show() \n",
    ")\n",
    "\n",
    "(\n",
    "    df\n",
    "    .orderBy(F.asc(\"continent\")) ## метод сортировки данных, обычно полезен для визуализации какой-то части данных в вашем выводе\n",
    "    .show()\n",
    ")\n",
    "\n",
    "(\n",
    "    df\n",
    "    .orderBy(F.asc(\"continent\"))\n",
    "    .explain(True)\n",
    ")\n",
    "\n",
    "## по плану запроса видно, что Physical Plan (физический движок с вызовом конкретных методов)\n",
    "## Важное замечание - тут вызывается Exchange rangepartitioning, который сильно хуже оптимизирован, чем Exchange hashpartitioning\n",
    "## Метод сортировки данных стоит использовать только для промежуточных вычислений, если в задаче обработки данных у вас\n",
    "## явно ничего нет про сортировку фрейма данных\n",
    "подробнее про виды partitioning стоит прочитать в доке по ссылкам выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d762c01d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner,Buffer(name))\n",
      ":- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "+- LogicalRDD [continent#259, country#260, name#261, population#262L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "name: string, continent: string, country: string, population: bigint, continent: string, country: string, population: bigint\n",
      "Project [name#8, continent#6, country#7, population#9L, continent#259, country#260, population#262L]\n",
      "+- Join Inner, (name#8 = name#261)\n",
      "   :- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "   +- LogicalRDD [continent#259, country#260, name#261, population#262L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [name#8, continent#6, country#7, population#9L, continent#259, country#260, population#262L]\n",
      "+- Join Inner, (name#8 = name#261)\n",
      "   :- Filter isnotnull(name#8)\n",
      "   :  +- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "   +- Filter isnotnull(name#261)\n",
      "      +- LogicalRDD [continent#259, country#260, name#261, population#262L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [name#8, continent#6, country#7, population#9L, continent#259, country#260, population#262L]\n",
      "   +- SortMergeJoin [name#8], [name#261], Inner\n",
      "      :- Sort [name#8 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(name#8, 400), ENSURE_REQUIREMENTS, [plan_id=438]\n",
      "      :     +- Filter isnotnull(name#8)\n",
      "      :        +- Scan ExistingRDD[continent#6,country#7,name#8,population#9L]\n",
      "      +- Sort [name#261 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(name#261, 400), ENSURE_REQUIREMENTS, [plan_id=439]\n",
      "            +- Filter isnotnull(name#261)\n",
      "               +- Scan ExistingRDD[continent#259,country#260,name#261,population#262L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## джойны и их планы запросов\n",
    "(\n",
    "    df\n",
    "    .join(\n",
    "        df,\n",
    "        on=[\"name\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .explain(True)\n",
    ")\n",
    "\n",
    "## по плану запроса видно, что Physical Plan (физический движок с вызовом конкретных методов)\n",
    "## Видим выбор SortMergeJoin подхода в движке\n",
    "## и предварительные сортировки ключей \"Sort [name#8 ASC NULLS FIRST], false, 0\" чтобы раскидать их по нодам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "401f6e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner,Buffer(name))\n",
      ":- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [continent#270, country#271, name#272, population#273L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "name: string, continent: string, country: string, population: bigint, continent: string, country: string, population: bigint\n",
      "Project [name#8, continent#6, country#7, population#9L, continent#270, country#271, population#273L]\n",
      "+- Join Inner, (name#8 = name#272)\n",
      "   :- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "   +- ResolvedHint (strategy=broadcast)\n",
      "      +- LogicalRDD [continent#270, country#271, name#272, population#273L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [name#8, continent#6, country#7, population#9L, continent#270, country#271, population#273L]\n",
      "+- Join Inner, (name#8 = name#272), rightHint=(strategy=broadcast)\n",
      "   :- Filter isnotnull(name#8)\n",
      "   :  +- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "   +- Filter isnotnull(name#272)\n",
      "      +- LogicalRDD [continent#270, country#271, name#272, population#273L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [name#8, continent#6, country#7, population#9L, continent#270, country#271, population#273L]\n",
      "   +- BroadcastHashJoin [name#8], [name#272], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(name#8)\n",
      "      :  +- Scan ExistingRDD[continent#6,country#7,name#8,population#9L]\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[2, string, false]),false), [plan_id=468]\n",
      "         +- Filter isnotnull(name#272)\n",
      "            +- Scan ExistingRDD[continent#270,country#271,name#272,population#273L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## джойны и их планы запросов\n",
    "(\n",
    "    df\n",
    "    .join(\n",
    "        F.broadcast(df),\n",
    "        on=[\"name\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .explain(True)\n",
    ")\n",
    "\n",
    "## по плану запроса видно, что Physical Plan (физический движок с вызовом конкретных методов)\n",
    "## Видим выбор BroadcastHashJoin, потому что к одному из фреймов явно применили Broadcast метод\n",
    "## про этот метод есть пояснение в лекции\n",
    "## также важно: видно что здесь нет этапа сортировки ключей, даже на втором фрейме данных\n",
    "## отсуствие этого этапа рассчета для джойна и дает нам оптимизацию по времени его работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7133436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, NOT (name#8 = name#283)\n",
      ":- SubqueryAlias a\n",
      ":  +- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- SubqueryAlias b\n",
      "      +- LogicalRDD [continent#281, country#282, name#283, population#284L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint, continent: string, country: string, name: string, population: bigint\n",
      "Join Inner, NOT (name#8 = name#283)\n",
      ":- SubqueryAlias a\n",
      ":  +- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- SubqueryAlias b\n",
      "      +- LogicalRDD [continent#281, country#282, name#283, population#284L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, NOT (name#8 = name#283), rightHint=(strategy=broadcast)\n",
      ":- Filter isnotnull(name#8)\n",
      ":  +- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "+- Filter isnotnull(name#283)\n",
      "   +- LogicalRDD [continent#281, country#282, name#283, population#284L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, Inner, NOT (name#8 = name#283)\n",
      "   :- Filter isnotnull(name#8)\n",
      "   :  +- Scan ExistingRDD[continent#6,country#7,name#8,population#9L]\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=491]\n",
      "      +- Filter isnotnull(name#283)\n",
      "         +- Scan ExistingRDD[continent#281,country#282,name#283,population#284L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## джойны и их планы запросов\n",
    "(\n",
    "    df.alias(\"a\")\n",
    "    .join(\n",
    "        F.broadcast(df.alias(\"b\")),\n",
    "        on=F.col(\"a.name\") != F.col(\"b.name\"),\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .explain(True)\n",
    ")\n",
    "\n",
    "## по плану запроса видно, что Physical Plan (физический движок с вызовом конкретных методов)\n",
    "## Видим выбор BroadcastNestedLoopJoin по причине того,\n",
    "## что условие джйона не вписывается в проверку ключей на равенство + применем Broadcast к одному из фреймов данных\n",
    "## Сортировки ключей тут также нет\n",
    "## Но join данных по частям на каждом воркере будет выполняться через вложенный цикл, из-за сложности условия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7c35452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, NOT (name#8 = name#343)\n",
      ":- SubqueryAlias a\n",
      ":  +- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "+- SubqueryAlias b\n",
      "   +- LogicalRDD [continent#341, country#342, name#343, population#344L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint, continent: string, country: string, name: string, population: bigint\n",
      "Join Inner, NOT (name#8 = name#343)\n",
      ":- SubqueryAlias a\n",
      ":  +- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "+- SubqueryAlias b\n",
      "   +- LogicalRDD [continent#341, country#342, name#343, population#344L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, NOT (name#8 = name#343)\n",
      ":- Filter isnotnull(name#8)\n",
      ":  +- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "+- Filter isnotnull(name#343)\n",
      "   +- LogicalRDD [continent#341, country#342, name#343, population#344L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CartesianProduct NOT (name#8 = name#343)\n",
      ":- *(1) Filter isnotnull(name#8)\n",
      ":  +- *(1) Scan ExistingRDD[continent#6,country#7,name#8,population#9L]\n",
      "+- *(2) Filter isnotnull(name#343)\n",
      "   +- *(2) Scan ExistingRDD[continent#341,country#342,name#343,population#344L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## джойны и их планы запросов\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") ## для след примера выключим автоматический Broadcast\n",
    "## небольших фреймов данных\n",
    "\n",
    "(\n",
    "    df.alias(\"a\")\n",
    "    .join(\n",
    "        df.alias(\"b\"),\n",
    "        on=F.col(\"a.name\") != F.col(\"b.name\"),\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .explain(True)\n",
    ")\n",
    "\n",
    "## по плану запроса видно, что Physical Plan (физический движок с вызовом конкретных методов)\n",
    "## Видим выбор CartesianProduct, самый \"долгий\" из физических подходов по вычислению работы джойна\n",
    "## В силу сложности условия джойна + невозможности применить Broadcast (мы его буквально отключили)\n",
    "## был выбран CartesianProduct метод\n",
    "## оптимизаций тут нет, данные распределяются на все возможные паросочетания ключей\n",
    "## раскидываются на ноды по хэшу и после проверяются по условию на правильно объединения данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17fca3e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('name_exp = s)\n",
      "+- Project [continent#6, country#7, name#8, population#9L, name_sep#375, name_exp#382]\n",
      "   +- Generate explode(name_sep#375), false, [name_exp#382]\n",
      "      +- Project [continent#6, country#7, name#8, population#9L, split(name#8, , -1) AS name_sep#375]\n",
      "         +- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint, name_sep: array<string>, name_exp: string\n",
      "Filter (name_exp#382 = s)\n",
      "+- Project [continent#6, country#7, name#8, population#9L, name_sep#375, name_exp#382]\n",
      "   +- Generate explode(name_sep#375), false, [name_exp#382]\n",
      "      +- Project [continent#6, country#7, name#8, population#9L, split(name#8, , -1) AS name_sep#375]\n",
      "         +- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (name_exp#382 = s)\n",
      "+- Generate explode(name_sep#375), false, [name_exp#382]\n",
      "   +- Project [continent#6, country#7, name#8, population#9L, split(name#8, , -1) AS name_sep#375]\n",
      "      +- Filter ((size(split(name#8, , -1), true) > 0) AND isnotnull(split(name#8, , -1)))\n",
      "         +- LogicalRDD [continent#6, country#7, name#8, population#9L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (name_exp#382 = s)\n",
      "+- *(1) Generate explode(name_sep#375), [continent#6, country#7, name#8, population#9L, name_sep#375], false, [name_exp#382]\n",
      "   +- *(1) Project [continent#6, country#7, name#8, population#9L, split(name#8, , -1) AS name_sep#375]\n",
      "      +- *(1) Filter ((size(split(name#8, , -1), true) > 0) AND isnotnull(split(name#8, , -1)))\n",
      "         +- *(1) Scan ExistingRDD[continent#6,country#7,name#8,population#9L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ниже план запроса методов split и explode\n",
    "## которые превращают строковую колонку в СПИСОК символов\n",
    "## а после список символов мы раскидываем на уникальные значения для каждой строки, тем самым\n",
    "## на каждый эл-т из списка создаем новую строку\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"name_sep\", F.split(\"name\", \"\"))\n",
    "    .withColumn(\"name_exp\", F.explode(\"name_sep\"))\n",
    "    .filter(F.col(\"name_exp\") == \"s\")\n",
    "    .explain(True)\n",
    ")\n",
    "\n",
    "## Логику отработки этого метода можно увидеть на, Physical Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46e6a7",
   "metadata": {},
   "source": [
    "## Сравнение реализаций задачи над RDD и DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40f5b580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('russia', 1), ('germany', 1), ('spain', 2), ('egypt', 2), ('france', 1)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## пример задачи через RDD на подсчет числа упоминаний стран в данных\n",
    "\n",
    "\n",
    "    rdd \\\n",
    "    .map(lambda x: x['country'].lower()) \\\n",
    "    .map( lambda x: (x, 1)) \\\n",
    "    .reduceByKey( lambda x,y: x + y) \\\n",
    "    .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d48cd208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['country], ['country, count(1) AS count(1)#466L]\n",
      "+- LogicalRDD [continent#189, country#190, name#191, population#192L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "country: string, count(1): bigint\n",
      "Aggregate [country#190], [country#190, count(1) AS count(1)#466L]\n",
      "+- LogicalRDD [continent#189, country#190, name#191, population#192L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [country#190], [country#190, count(1) AS count(1)#466L]\n",
      "+- Project [country#190]\n",
      "   +- LogicalRDD [continent#189, country#190, name#191, population#192L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[country#190], functions=[count(1)], output=[country#190, count(1)#466L])\n",
      "   +- Exchange hashpartitioning(country#190, 400), ENSURE_REQUIREMENTS, [plan_id=547]\n",
      "      +- HashAggregate(keys=[country#190], functions=[partial_count(1)], output=[country#190, count#470L])\n",
      "         +- Project [country#190]\n",
      "            +- Scan ExistingRDD[continent#189,country#190,name#191,population#192L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## она же через DataFrame API\n",
    "(\n",
    "    df\n",
    "    .groupBy(\"country\")\n",
    "    .agg(F.count(\"*\"))\n",
    "    .explain(True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322f261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3_9",
   "language": "python",
   "name": "python_3_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
