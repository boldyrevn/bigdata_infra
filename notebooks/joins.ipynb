{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36a7be1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T13:12:48.641381Z",
     "start_time": "2024-05-29T13:12:44.995399Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67c84f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:49:30.067854Z",
     "start_time": "2024-05-29T12:49:30.041662Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-03T07:26:15.712552Z",
     "iopub.status.busy": "2023-08-03T07:26:15.711446Z",
     "iopub.status.idle": "2023-08-03T07:26:15.736711Z",
     "shell.execute_reply": "2023-08-03T07:26:15.735507Z",
     "shell.execute_reply.started": "2023-08-03T07:26:15.712498Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18653/4223674425.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML, clear_output\n",
      "/tmp/ipykernel_18653/4223674425.py:1: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML, clear_output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.prompt { min-width:10ex !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>div#notebook { font-size:12px !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML, clear_output\n",
    "display(HTML('<style>.container { width:90% !important; }</style>'))\n",
    "display(HTML('<style>.prompt { min-width:10ex !important; }</style>'))\n",
    "display(HTML('<style>div#notebook { font-size:12px !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfdfd8ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:49:30.229431Z",
     "start_time": "2024-05-29T12:49:30.223135Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-03T07:26:16.061426Z",
     "iopub.status.busy": "2023-08-03T07:26:16.060784Z",
     "iopub.status.idle": "2023-08-03T07:26:16.070597Z",
     "shell.execute_reply": "2023-08-03T07:26:16.069272Z",
     "shell.execute_reply.started": "2023-08-03T07:26:16.061382Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0929e4",
   "metadata": {},
   "source": [
    "## Spark Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d5308a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:49:44.864833Z",
     "start_time": "2024-05-29T12:49:44.859846Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-03T07:26:18.247121Z",
     "iopub.status.busy": "2023-08-03T07:26:18.246000Z",
     "iopub.status.idle": "2023-08-03T07:26:18.252886Z",
     "shell.execute_reply": "2023-08-03T07:26:18.251653Z",
     "shell.execute_reply.started": "2023-08-03T07:26:18.247067Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session_name = 'joins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a941e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:49:48.946290Z",
     "start_time": "2024-05-29T12:49:45.327923Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-03T07:26:18.660752Z",
     "iopub.status.busy": "2023-08-03T07:26:18.659923Z",
     "iopub.status.idle": "2023-08-03T07:26:20.042243Z",
     "shell.execute_reply": "2023-08-03T07:26:20.040563Z",
     "shell.execute_reply.started": "2023-08-03T07:26:18.660711Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mtspark import kinit\n",
    "\n",
    "kinit(principal='iagalper@MSK.MTS.RU', keytab='/home/iagalper/iagalper.keytab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "862b0c6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:50:29.726346Z",
     "start_time": "2024-05-29T12:49:48.951264Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-03T07:26:20.045952Z",
     "iopub.status.busy": "2023-08-03T07:26:20.045160Z",
     "iopub.status.idle": "2023-08-03T07:26:52.804847Z",
     "shell.execute_reply": "2023-08-03T07:26:52.803292Z",
     "shell.execute_reply.started": "2023-08-03T07:26:20.045907Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: mtspark.version\n",
      "Warning: Ignoring non-Spark config property: mtspark.python.interpreter\n",
      "Warning: Ignoring non-Spark config property: mtspark.python.path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: file = /home/iagalper/adv_venvs/py39-spark3.5/lib/python3.9/site-packages/mtspark/config/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/29 15:49:56 WARN Utils: Service 'sparkDriver' could not bind on port 40000. Attempting port 40001.\n",
      "24/05/29 15:49:56 WARN Utils: Service 'sparkDriver' could not bind on port 40001. Attempting port 40002.\n",
      "24/05/29 15:49:56 WARN Utils: Service 'sparkDriver' could not bind on port 40002. Attempting port 40003.\n",
      "24/05/29 15:49:56 WARN Utils: Service 'sparkDriver' could not bind on port 40003. Attempting port 40004.\n",
      "24/05/29 15:49:56 WARN Utils: Service 'sparkDriver' could not bind on port 40004. Attempting port 40005.\n",
      "24/05/29 15:49:56 WARN Utils: Service 'sparkDriver' could not bind on port 40005. Attempting port 40006.\n",
      "24/05/29 15:49:56 WARN Utils: Service 'sparkDriver' could not bind on port 40006. Attempting port 40007.\n",
      "24/05/29 15:49:56 WARN Utils: Service 'sparkDriver' could not bind on port 40007. Attempting port 40008.\n",
      "24/05/29 15:49:56 WARN Utils: Service 'sparkDriver' could not bind on port 40008. Attempting port 40009.\n",
      "24/05/29 15:49:56 WARN Utils: Service 'sparkDriver' could not bind on port 40009. Attempting port 40010.\n",
      "24/05/29 15:50:00 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "24/05/29 15:50:01 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/05/29 15:50:20 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40501. Attempting port 40502.\n",
      "24/05/29 15:50:20 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40502. Attempting port 40503.\n",
      "24/05/29 15:50:20 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40503. Attempting port 40504.\n",
      "24/05/29 15:50:20 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40504. Attempting port 40505.\n",
      "24/05/29 15:50:20 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40505. Attempting port 40506.\n",
      "24/05/29 15:50:20 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40506. Attempting port 40507.\n",
      "24/05/29 15:50:20 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40507. Attempting port 40508.\n",
      "24/05/29 15:50:20 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40508. Attempting port 40509.\n",
      "24/05/29 15:50:20 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40509. Attempting port 40510.\n",
      "24/05/29 15:50:20 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40510. Attempting port 40511.\n",
      "24/05/29 15:50:21 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "from mtspark import get_spark\n",
    "\n",
    "spark = get_spark({\n",
    "    \"spark.app.name\": session_name,\n",
    "    \"spark.driver.memory\": \"2g\",\n",
    "    \"spark.driver.maxResultSize\": \"2g\",\n",
    "    \"spark.executor.cores\": \"4\",\n",
    "    \"spark.executor.memory\": \"2g\",\n",
    "    \"spark.ui.showConsoleProgress\": False\n",
    "}, spark_version=\"3.5.0\")\n",
    "\n",
    "print('\\nsuccess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587a1263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:50:29.734009Z",
     "start_time": "2024-05-29T12:50:29.729130Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-03T07:26:52.807470Z",
     "iopub.status.busy": "2023-08-03T07:26:52.806958Z",
     "iopub.status.idle": "2023-08-03T07:26:52.847224Z",
     "shell.execute_reply": "2023-08-03T07:26:52.845844Z",
     "shell.execute_reply.started": "2023-08-03T07:26:52.807418Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession, Column\n",
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e23d8b51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:50:29.741862Z",
     "start_time": "2024-05-29T12:50:29.737818Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-03T07:26:52.851146Z",
     "iopub.status.busy": "2023-08-03T07:26:52.850111Z",
     "iopub.status.idle": "2023-08-03T07:26:52.856607Z",
     "shell.execute_reply": "2023-08-03T07:26:52.855304Z",
     "shell.execute_reply.started": "2023-08-03T07:26:52.851098Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15feeeb7",
   "metadata": {},
   "source": [
    "### tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da8df82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:50:31.511477Z",
     "start_time": "2024-05-29T12:50:29.743994Z"
    }
   },
   "outputs": [],
   "source": [
    "df_big = spark.createDataFrame(\n",
    "    data=[\n",
    "        (\"яблоко\", 1, [1,2], datetime.today().date()), \n",
    "        (\"груша\", 2, [3,4], datetime.today().date()),\n",
    "        (\"арбуз\", 3, [4,5], datetime.today().date()),\n",
    "        (\"огурец\", 4, [6,7], datetime.today().date() - timedelta(1)),\n",
    "        (\"помидор\", 5, [8,9], datetime.today().date() - timedelta(1)),\n",
    "        (\"перец\", 6, [2,3], datetime.today().date() - timedelta(1)),\n",
    "        (\"редис\", 7, [5,6], datetime.today().date() - timedelta(2)),\n",
    "        (\"апельсин\", 8, [7,8], datetime.today().date() - timedelta(2)),\n",
    "        (\"лимон\", 9, [1,9], datetime.today().date() - timedelta(2)),\n",
    "        (\"тыква\", 10, [2,7], datetime.today().date() - timedelta(3))\n",
    "    ], \n",
    "    schema = [\"name\", \"count\", \"array\", \"date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b7b67d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:50:40.249897Z",
     "start_time": "2024-05-29T12:50:31.514892Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+----------+\n",
      "|name    |count|array |date      |\n",
      "+--------+-----+------+----------+\n",
      "|яблоко  |1    |[1, 2]|2024-05-29|\n",
      "|груша   |2    |[3, 4]|2024-05-29|\n",
      "|арбуз   |3    |[4, 5]|2024-05-29|\n",
      "|огурец  |4    |[6, 7]|2024-05-28|\n",
      "|помидор |5    |[8, 9]|2024-05-28|\n",
      "|перец   |6    |[2, 3]|2024-05-28|\n",
      "|редис   |7    |[5, 6]|2024-05-27|\n",
      "|апельсин|8    |[7, 8]|2024-05-27|\n",
      "|лимон   |9    |[1, 9]|2024-05-27|\n",
      "|тыква   |10   |[2, 7]|2024-05-26|\n",
      "+--------+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_big.show(20,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb4a180b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:50:40.328414Z",
     "start_time": "2024-05-29T12:50:40.252589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7292d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7b44e7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:51:00.289093Z",
     "start_time": "2024-05-29T12:51:00.241286Z"
    }
   },
   "outputs": [],
   "source": [
    "df_small = spark.createDataFrame(\n",
    "    data=[\n",
    "        (\"яблоко\", 11, [3,4], datetime.today().date()), \n",
    "        (\"груша\", 12, [1,2], datetime.today().date() - timedelta(1)),\n",
    "        (\"арбуз\", 13, [1,3], datetime.today().date() - timedelta(2)),\n",
    "        (\"гриб\", 14, [4,5], datetime.today().date() - timedelta(3)),\n",
    "        (\"хлеб\", 15, [1,8], datetime.today().date() - timedelta(4))\n",
    "    ], \n",
    "    schema = [\"name\", \"count\", \"array\", \"date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72816fda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:51:05.834299Z",
     "start_time": "2024-05-29T12:51:00.665160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+----------+\n",
      "|name  |count|array |date      |\n",
      "+------+-----+------+----------+\n",
      "|яблоко|11   |[3, 4]|2024-05-29|\n",
      "|груша |12   |[1, 2]|2024-05-28|\n",
      "|арбуз |13   |[1, 3]|2024-05-27|\n",
      "|гриб  |14   |[4, 5]|2024-05-26|\n",
      "|хлеб  |15   |[1, 8]|2024-05-25|\n",
      "+------+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small.show(20,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe88bae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:51:05.849647Z",
     "start_time": "2024-05-29T12:51:05.842373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923764b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff23e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:51:05.885442Z",
     "start_time": "2024-05-29T12:51:05.852274Z"
    }
   },
   "outputs": [],
   "source": [
    "df_big_1 = df_big.repartition(1)\n",
    "df_small_1 = df_small.repartition(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7080e4",
   "metadata": {},
   "source": [
    "### equi-join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764426e",
   "metadata": {},
   "source": [
    "#### 200 партиций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73140f80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:53:21.410171Z",
     "start_time": "2024-05-29T12:53:21.334985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [name#0], [name#25], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(name#0)\n",
      "   :  +- Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=111]\n",
      "      +- Filter isnotnull(name#25)\n",
      "         +- Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# применится броадкаст, т.к. есть hint и inner-join\n",
    "\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        sf.broadcast(df_small.alias(\"df_small\")), \n",
    "        (df_big.name == df_small.name),\n",
    "        \"inner\"\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9d05117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:53:55.727665Z",
     "start_time": "2024-05-29T12:53:55.665054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [name#0], [name#25], LeftOuter, BuildRight, false\n",
      "   :- Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=129]\n",
      "      +- Filter isnotnull(name#25)\n",
      "         +- Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# применится броадкаст, т.к. есть hint и lef-join\n",
    "\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        sf.broadcast(df_small.alias(\"df_small\")), \n",
    "        (df_big.name == df_small.name),\n",
    "        \"left\"\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5b7fb21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:54:11.487113Z",
     "start_time": "2024-05-29T12:54:11.377463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [name#0], [name#25], FullOuter\n",
      "   :- Sort [name#0 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(name#0, 200), ENSURE_REQUIREMENTS, [plan_id=144]\n",
      "   :     +- Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "   +- Sort [name#25 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(name#25, 200), ENSURE_REQUIREMENTS, [plan_id=145]\n",
      "         +- Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# здесь будет sort merge, несмотря на hint, т.к. используем full-join\n",
    "\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        sf.broadcast(df_small.alias(\"df_small\")), \n",
    "        (df_big.name == df_small.name),\n",
    "        \"full\"\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9f2176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:54:23.820630Z",
     "start_time": "2024-05-29T12:54:23.743926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ShuffledHashJoin [name#0], [name#25], Inner, BuildRight\n",
      "   :- Exchange hashpartitioning(name#0, 200), ENSURE_REQUIREMENTS, [plan_id=170]\n",
      "   :  +- Filter isnotnull(name#0)\n",
      "   :     +- Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "   +- Exchange hashpartitioning(name#25, 200), ENSURE_REQUIREMENTS, [plan_id=171]\n",
      "      +- Filter isnotnull(name#25)\n",
      "         +- Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# форсим применение shuffle hash, можно не выставлять spark.sql.join.preferSortMergeJoin в false, т.к. у нас есть hint\n",
    "\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        df_small.alias(\"df_small\").hint('SHUFFLE_HASH'), \n",
    "        (df_big.name == df_small.name),\n",
    "        \"inner\"\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b925487a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:54:33.816803Z",
     "start_time": "2024-05-29T12:54:33.768927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ShuffledHashJoin [name#0], [name#25], FullOuter, BuildRight\n",
      "   :- Exchange hashpartitioning(name#0, 200), ENSURE_REQUIREMENTS, [plan_id=186]\n",
      "   :  +- Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "   +- Exchange hashpartitioning(name#25, 200), ENSURE_REQUIREMENTS, [plan_id=187]\n",
      "      +- Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shuffle hash работает и с full-join\n",
    "\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        df_small.alias(\"df_small\").hint('SHUFFLE_HASH'), \n",
    "        (df_big.name == df_small.name),\n",
    "        \"full\"\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f665f029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:54:56.802083Z",
     "start_time": "2024-05-29T12:54:56.760560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [name#0], [name#25], FullOuter\n",
      "   :- Sort [name#0 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(name#0, 200), ENSURE_REQUIREMENTS, [plan_id=202]\n",
      "   :     +- Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "   +- Sort [name#25 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(name#25, 200), ENSURE_REQUIREMENTS, [plan_id=203]\n",
      "         +- Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# а тут Sort Merge, т.к. нет hint\n",
    "\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        df_small.alias(\"df_small\"), \n",
    "        (df_big.name == df_small.name),\n",
    "        \"full\"\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34119d3f",
   "metadata": {},
   "source": [
    "#### 1 партиция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f1a53ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:55:24.320364Z",
     "start_time": "2024-05-29T12:55:24.229087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [name#0], [name#25], Inner\n",
      "   :- Sort [name#0 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange SinglePartition, REPARTITION_BY_NUM, [plan_id=229]\n",
      "   :     +- Filter isnotnull(name#0)\n",
      "   :        +- Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "   +- Sort [name#25 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange SinglePartition, REPARTITION_BY_NUM, [plan_id=232]\n",
      "         +- Filter isnotnull(name#25)\n",
      "            +- Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# применится броадкаст, т.к. анализатор понял, что таблички маленькие\n",
    "\n",
    "df_big_1.alias('df_big_1')\\\n",
    "    .join(\n",
    "        df_small_1.alias(\"df_small_1\"), \n",
    "        (df_big_1.name == df_small_1.name),\n",
    "        \"inner\"\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f4a95",
   "metadata": {},
   "source": [
    "### non equi-join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfeb5cb",
   "metadata": {},
   "source": [
    "#### 200 партиций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "734d70c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:55:50.116944Z",
     "start_time": "2024-05-29T12:55:50.053406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CartesianProduct NOT (name#0 = name#25)\n",
      ":- *(1) Filter isnotnull(name#0)\n",
      ":  +- *(1) Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "+- *(2) Filter isnotnull(name#25)\n",
      "   +- *(2) Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cartesian product, т.к. нет ни одного ключа на равенство\n",
    "\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        df_small.alias(\"df_small\"), \n",
    "        (df_big.name != df_small.name)\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf00467d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:56:02.597168Z",
     "start_time": "2024-05-29T12:56:02.548507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, FullOuter, NOT (name#0 = name#25)\n",
      "   :- Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=273]\n",
      "      +- Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# brodcast nested loop, т.к. cartesian product не поддерживает outer джойны\n",
    "\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        df_small.alias(\"df_small\"), \n",
    "        (df_big.name != df_small.name), \n",
    "        'full'\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ed91911",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:56:29.148214Z",
     "start_time": "2024-05-29T12:56:29.096612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [name#0], [name#25], FullOuter, NOT (date#3 = date#28)\n",
      "   :- Sort [name#0 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(name#0, 200), ENSURE_REQUIREMENTS, [plan_id=288]\n",
      "   :     +- Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "   +- Sort [name#25 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(name#25, 200), ENSURE_REQUIREMENTS, [plan_id=289]\n",
      "         +- Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort merge, т.к. есть одно условие на равенство ключей\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        df_small.alias(\"df_small\"), \n",
    "        (\n",
    "            (df_big.name == df_small.name) & (df_big.date != df_small.date)\n",
    "        ),\n",
    "        'full'\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0863fb83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:58:09.623902Z",
     "start_time": "2024-05-29T12:58:09.565191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CartesianProduct (size(array_except(array#2, array#27), true) > 0)\n",
      ":- *(1) Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "+- *(2) Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cartesian product, т.к. нет ни одного ключа на равенство\n",
    "\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        df_small.alias(\"df_small\"), \n",
    "        (\n",
    "            sf.size(sf.array_except(df_big.array, df_small.array)) > 0\n",
    "        )\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7211ce10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:58:28.904084Z",
     "start_time": "2024-05-29T12:58:28.852664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastNestedLoopJoin BuildRight, FullOuter, (size(array_except(array#2, array#27), true) > 0)\n",
      "   :- Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "   +- BroadcastExchange IdentityBroadcastMode, [plan_id=319]\n",
      "      +- Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# brodcast nested loop, т.к. нет ни одного ключа на равенство и есть hint на broadcast\n",
    "\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        sf.broadcast(df_small.alias(\"df_small\")), \n",
    "        (\n",
    "            sf.size(sf.array_except(df_big.array, df_small.array)) > 0\n",
    "        ),\n",
    "        'full'\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c344dd54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:58:53.099401Z",
     "start_time": "2024-05-29T12:58:53.043599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [array#2], [array#27], Inner\n",
      "   :- Sort [array#2 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(array#2, 200), ENSURE_REQUIREMENTS, [plan_id=342]\n",
      "   :     +- Filter isnotnull(array#2)\n",
      "   :        +- Scan ExistingRDD[name#0,count#1L,array#2,date#3]\n",
      "   +- Sort [array#27 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(array#27, 200), ENSURE_REQUIREMENTS, [plan_id=343]\n",
      "         +- Filter isnotnull(array#27)\n",
      "            +- Scan ExistingRDD[name#25,count#26L,array#27,date#28]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort merge, т.к. как минимум с 3 версии спарка, массивы сортируемы\n",
    "\n",
    "df_big.alias('df_big')\\\n",
    "    .join(\n",
    "        df_small.alias(\"df_small\"), \n",
    "        (\n",
    "            (df_big.array == df_small.array)\n",
    "        )\n",
    "    )\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0c099",
   "metadata": {},
   "source": [
    "#### 1 партиция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb063d23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T10:37:19.374982Z",
     "start_time": "2024-05-29T10:37:12.855963Z"
    }
   },
   "outputs": [],
   "source": [
    "# brodcast nested loop, т.к. анализатор понял, что таблички маленькие\n",
    "\n",
    "df_big_1.alias('df_big_1')\\\n",
    "    .join(\n",
    "        df_small_1.alias(\"df_small_1\"), \n",
    "        (\n",
    "            F.size(F.array_except(df_big_1.array, df_small_1.array)) > 0\n",
    "        ),\n",
    "        'full'\n",
    "    )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e35e90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T10:37:57.438348Z",
     "start_time": "2024-05-29T10:37:52.262832Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb2d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
